{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "0SCUZq8R27Nv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75f911fd-b8aa-48b0-9751-56f5680b557c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tarfile\n",
        "data_dir='/content/drive/MyDrive/ColabNotebooks/refcocog.tar.gz'\n",
        "# Extract data\n",
        "tar = tarfile.open(data_dir)\n",
        "tar.extractall('dataset/')\n"
      ],
      "metadata": {
        "id": "8Bum1NKU2v5a"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "LdnLcbFyfkvW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef235d89-30b1-43ea-bffa-e2657949990a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.1.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.16.0+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.23.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.31.0)\n",
            "Requirement already satisfied: torch==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.1.0+cu121)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchvision) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchvision) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchvision) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchvision) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchvision) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchvision) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchvision) (2.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2023.11.17)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.1.0->torchvision) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.1.0->torchvision) (1.3.0)\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement SummaryWriter (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for SummaryWriter\u001b[0m\u001b[31m\n",
            "\u001b[0mRequirement already satisfied: tensorboard in /usr/local/lib/python3.10/dist-packages (2.15.1)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.60.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (3.5.1)\n",
            "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.23.5)\n",
            "Requirement already satisfied: protobuf<4.24,>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (3.20.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (2.31.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (67.7.2)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.16.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (3.0.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard) (2023.11.17)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard) (2.1.3)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard) (0.5.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard) (3.2.2)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.10/dist-packages (0.19.3)\n",
            "Requirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from scikit-image) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image) (1.11.4)\n",
            "Requirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.10/dist-packages (from scikit-image) (3.2.1)\n",
            "Requirement already satisfied: pillow!=7.1.0,!=7.1.1,!=8.3.0,>=6.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-image) (9.4.0)\n",
            "Requirement already satisfied: imageio>=2.4.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image) (2.31.6)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.10/dist-packages (from scikit-image) (2023.12.9)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image) (1.5.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from scikit-image) (23.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.46.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.5)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (23.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
            "Requirement already satisfied: Lambda in /usr/local/lib/python3.10/dist-packages (0.0.1)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (4.8.0.76)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.10/dist-packages (from opencv-python) (1.23.5)\n",
            "Requirement already satisfied: torchmetrics in /usr/local/lib/python3.10/dist-packages (1.2.1)\n",
            "Requirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (1.23.5)\n",
            "Requirement already satisfied: packaging>17.1 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (23.2)\n",
            "Requirement already satisfied: torch>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (2.1.0+cu121)\n",
            "Requirement already satisfied: lightning-utilities>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (0.10.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (67.7.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (3.13.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8.1->torchmetrics) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.8.1->torchmetrics) (1.3.0)\n",
            "Collecting pyyaml==5.1\n",
            "  Using cached PyYAML-5.1.tar.gz (274 kB)\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
            "\u001b[1;36mhint\u001b[0m: See above for details.\n"
          ]
        }
      ],
      "source": [
        "!pip install torch\n",
        "!pip install torchvision\n",
        "!pip install SummaryWriter\n",
        "!pip install tensorboard\n",
        "!pip install scikit-image\n",
        "!pip install matplotlib\n",
        "!pip install Lambda\n",
        "!pip install opencv-python\n",
        "!pip install torchmetrics\n",
        "\n",
        "!python -m pip install pyyaml==5.1\n",
        "import sys, os, distutils.core\n",
        "\n",
        "\n",
        "\n",
        "import torch\n",
        "torch.cuda.current_device()\n",
        "import torchvision\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as T\n",
        "from torchvision import ops\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from posixpath import split\n",
        "import json\n",
        "import tarfile\n",
        "import io\n",
        "import pickle\n",
        "from PIL import Image\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import jaccard_score\n",
        "\n",
        "\n",
        "\n",
        "# import some common libraries\n",
        "import cv2\n",
        "#from google.colab.patches import cv2_imshow\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "temperature = 1.0\n",
        "torch.autograd.set_detect_anomaly(True)\n",
        "data = {}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "LZum46_RLQ44",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e8d8c10e-cd28-4dd8-ace6-c77db5de995a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/openai/CLIP.git\n",
            "  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-d8bq1xnh\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-d8bq1xnh\n",
            "  Resolved https://github.com/openai/CLIP.git to commit a1d071733d7111c9c014f024669f959182114e33\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (6.1.3)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (4.66.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (2.1.0+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (0.16.0+cu121)\n",
            "Requirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /usr/local/lib/python3.10/dist-packages (from ftfy->clip==1.0) (0.2.12)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (2.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (1.23.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (2.31.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->clip==1.0) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->clip==1.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->clip==1.0) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->clip==1.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->clip==1.0) (2023.11.17)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->clip==1.0) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install git+https://github.com/openai/CLIP.git\n",
        "from clip import clip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "PxhqoAtfkLaI"
      },
      "outputs": [],
      "source": [
        "def get_datasets(refs, train = True):\n",
        "  ref_data = []\n",
        "  for val in refs:\n",
        "    if train:\n",
        "      if val['split'] == 'train' or val['split'] == 'val':\n",
        "        ref_data.append(val)\n",
        "    else:\n",
        "      if val['split'] == 'test':\n",
        "        ref_data.append(val)\n",
        "\n",
        "  '''for i,v in enumerate(datasets):\n",
        "    training_data, test_data = v'''\n",
        "\n",
        "  return ref_data\n",
        "\n",
        "\n",
        "def preProcess_datasets(data_dir):\n",
        "  train_dataset = {}\n",
        "  test_dataset = {}\n",
        "  f = open(f'{data_dir}/annotations/refs(umd).p', 'rb')\n",
        "  data['refs'] = pickle.load(f)\n",
        "  '''if train:\n",
        "    refs = get_datasets(data['refs'], train=True)\n",
        "  else:\n",
        "    refs = get_datasets(data['refs'], train=False)'''\n",
        "  instances_file = os.path.join(f'{data_dir}/annotations/instances.json')\n",
        "  instances = json.load(open(instances_file, 'r'))\n",
        "  data['images'] = instances['images']\n",
        "  data['annotations'] = instances['annotations']\n",
        "  #print(data['refs'][0])\n",
        "  #print(data['images'][0])\n",
        "  #print(data['annotations'][0])\n",
        "  c=0\n",
        "  d=0\n",
        "  for key,val in enumerate(data['refs']):\n",
        "    for v in data['images']:\n",
        "      if val['image_id'] == v['id']:\n",
        "        fname = os.path.join(data_dir+'/images/', v['file_name'])\n",
        "        if os.path.exists(fname):\n",
        "          if val['split'] == 'train' or val['split'] == 'val':\n",
        "            train_dataset[c] = {}\n",
        "            train_dataset[c]['image'] = v['file_name']\n",
        "            train_dataset[c]['captions'] = [t['raw'] for t in val['sentences']]\n",
        "            train_dataset[c]['id'] = val['image_id']\n",
        "            break\n",
        "          else:\n",
        "            test_dataset[d] = {}\n",
        "            test_dataset[d]['image'] = v['file_name']\n",
        "            test_dataset[d]['captions'] = [t['raw'] for t in val['sentences']]\n",
        "            test_dataset[d]['id'] = val['image_id']\n",
        "            break\n",
        "    for ann in data['annotations']:\n",
        "      if val['ann_id'] == ann['id']:\n",
        "        if val['split'] == 'train' or val['split'] == 'val':\n",
        "          train_dataset[c]['bbox'] = ann['bbox']\n",
        "          c+=1\n",
        "          break\n",
        "        else:\n",
        "          test_dataset[d]['bbox'] = ann['bbox']\n",
        "          d+=1\n",
        "          break\n",
        "    if c>=5000 and d>=5000:\n",
        "      break\n",
        "\n",
        "  return train_dataset, test_dataset\n",
        "\n",
        "\n",
        "def resize_bounding_box(bbox, original_size, new_size):\n",
        "  x_min, y_min, x_max, y_max = bbox\n",
        "  orig_width, orig_height = original_size\n",
        "  new_width, new_height = new_size\n",
        "\n",
        "  # Calculate the scaling factors\n",
        "  width_ratio = new_width / orig_width\n",
        "  height_ratio = new_height / orig_height\n",
        "\n",
        "  # Resize the bounding box coordinates\n",
        "  new_x_min = int(x_min * width_ratio)\n",
        "  new_y_min = int(y_min * height_ratio)\n",
        "  new_x_max = int(x_max * width_ratio)\n",
        "  new_y_max = int(y_max * height_ratio)\n",
        "\n",
        "  return new_x_min, new_y_min, new_x_max, new_y_max\n",
        "\n",
        "def xywh_to_xyxy(boxes):\n",
        "  xmin = boxes[0]\n",
        "  ymin = boxes[1]\n",
        "  w = boxes[2]\n",
        "  h = boxes[3]\n",
        "  xmax = xmin + w\n",
        "  ymax = ymin + h\n",
        "  return [xmin, ymin, xmax, ymax]\n",
        "\n",
        "\n",
        "\n",
        "def visualise_result(img, txt, bbox):\n",
        "  resize_image = T.Resize(100)\n",
        "  img = resize_image(img)\n",
        "  convert_tensor = T.ToTensor()\n",
        "  image = convert_tensor(img).to(device)\n",
        "  image = image.clone().detach()\n",
        "  image = image.type(torch.ByteTensor).to(device)\n",
        "  boxs = torch.tensor(bbox,dtype=torch.int).to(device)\n",
        "  boxes = boxs.reshape([1,4])\n",
        "  boxes = torchvision.ops.box_convert(boxes, \"cxcywh\", \"xyxy\").to(device)\n",
        "  image = draw_bounding_boxes(image=image, boxes=boxes, width=2, colors=(0,0,255), fill=True).to(device)\n",
        "  image = image.permute(1,2,0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "2b4mMLT8vbdE"
      },
      "outputs": [],
      "source": [
        "class RefCOCOgDataset(Dataset):\n",
        "    def __init__(self, dataset, transform, target_transform, data_dir='dataset/refcocog'):\n",
        "        super(RefCOCOgDataset, self).__init__()\n",
        "        self.data_dir = data_dir\n",
        "        self.transform = transform\n",
        "        self.target_transform = target_transform\n",
        "        self.dataset = dataset\n",
        "\n",
        "\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        data_item = {}\n",
        "        fname = os.path.join(self.data_dir+'/images/', self.dataset[idx]['image'])\n",
        "        image = Image.open(fname).convert('RGB')\n",
        "        shape_0 = list(image.size)\n",
        "        if self.transform:\n",
        "            data_item['image'] = self.transform(image)\n",
        "        if self.target_transform:\n",
        "            data_item['captions'] = clip.tokenize([sent for desc in self.dataset[idx]['captions'] for sent in desc])\n",
        "            bb = xywh_to_xyxy(self.dataset[idx]['bbox'])\n",
        "            bbox = resize_bounding_box(bb, shape_0, [224,224])\n",
        "            data_item['bbox'] = torch.tensor(bbox)\n",
        "        return data_item\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "nB6aV1uyGAxd"
      },
      "outputs": [],
      "source": [
        "def get_cost_function():\n",
        "  #cost_function = torch.nn.SmoothL1Loss()\n",
        "  cost_function = torch.nn.SmoothL1Loss()\n",
        "  return cost_function\n",
        "\n",
        "def get_iou_torch(ground_truth, pred):\n",
        "  # Coordinates of the area of intersection.\n",
        "\n",
        "  ix1 = torch.max(ground_truth[0][0], pred[0][0])\n",
        "  iy1 = torch.max(ground_truth[0][1], pred[0][1])\n",
        "  ix2 = torch.min(ground_truth[0][2], pred[0][2])\n",
        "  iy2 = torch.min(ground_truth[0][3], pred[0][3])\n",
        "\n",
        "  # Intersection height and width.\n",
        "  i_height = torch.max(iy2 - iy1 + 1, torch.tensor(0.))\n",
        "  i_width = torch.max(ix2 - ix1 + 1, torch.tensor(0.))\n",
        "\n",
        "  area_of_intersection = i_height * i_width\n",
        "\n",
        "  # Ground Truth dimensions.\n",
        "  gt_height = ground_truth[0][3] - ground_truth[0][1] + 1\n",
        "  gt_width = ground_truth[0][2] - ground_truth[0][0] + 1\n",
        "\n",
        "  # Prediction dimensions.\n",
        "  pd_height = pred[0][3] - pred[0][1] + 1\n",
        "  pd_width = pred[0][2] - pred[0][0] + 1\n",
        "\n",
        "  area_of_union = gt_height * gt_width + pd_height * pd_width - area_of_intersection\n",
        "\n",
        "  iou = (area_of_intersection + 1e-5) / (area_of_union + 1e-5) # Adding a small epsilon to avoid division by zero\n",
        "  #average_iou = torch.mean(iou)\n",
        "\n",
        "  return iou.detach().cpu().numpy()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "XFYNX6i_GA-O"
      },
      "outputs": [],
      "source": [
        "def get_optimizer(model, lr, wd, momentum):\n",
        "  optimizer = torch.optim.SGD([\n",
        "      {'params': model.parameters(), 'lr': lr}\n",
        "  ], lr=lr, weight_decay=wd, momentum=momentum)\n",
        "\n",
        "  #optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "  return optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "TIeelz-AF32o"
      },
      "outputs": [],
      "source": [
        "def training_step(net, data_loader, optimizer, cost_function, device='cuda'):\n",
        "    samples = 0.0\n",
        "    cumulative_loss = 0.0\n",
        "    cumulative_accuracy = 0.0\n",
        "    batch_ious = []\n",
        "    # set the network to training mode\n",
        "    net = net.float()\n",
        "    net.train()\n",
        "\n",
        "  # iterate over the training set\n",
        "    for batch_idx, batch in enumerate(data_loader):\n",
        "\n",
        "      #load data into GPU\n",
        "      batch['image'] = batch['image'].to(device)\n",
        "      batch['captions'] = batch['captions'].to(device)\n",
        "      target_bbox = batch['bbox'].to(device)\n",
        "      # forward pass\n",
        "      out_bbox = net(batch['image'], batch['captions'])\n",
        "\n",
        "      # loss computation\n",
        "      loss = cost_function(out_bbox, target_bbox)\n",
        "      # fetch prediction and loss value\n",
        "      # backward pass\n",
        "      loss.requires_grad = True\n",
        "      loss.backward()\n",
        "\n",
        "      # parameters update\n",
        "      optimizer.step()\n",
        "\n",
        "      # gradients reset\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      # fetch prediction and loss value\n",
        "      samples += batch['bbox'].shape[0]\n",
        "      cumulative_loss += loss.item()\n",
        "\n",
        "\n",
        "\n",
        "      #iou_accuracy = get_iou_torch(target_bbox, out_bbox)\n",
        "      iou_accuracy = torchvision.ops.box_iou(target_bbox, out_bbox).detach().cpu().numpy().sum().item()\n",
        "      #print(iou.detach().cpu().numpy())\n",
        "      #print(print(iou.detach().cpu().numpy().sum().item()))\n",
        "      #iou_accuracy = iou_pytorch(out_bbox, target_bbox)\n",
        "      #batch_ious.append(iou_accuracy.sum().item())\n",
        "      batch_ious.append(iou_accuracy)\n",
        "      #cumulative_accuracy += iou_accuracy\n",
        "\n",
        "      # Calculate the average IoU across all batches\n",
        "    if batch_ious:\n",
        "        # Calculate the average IoU across all batches\n",
        "        cumulative_accuracy = np.mean(batch_ious)\n",
        "    else:\n",
        "        cumulative_accuracy = 0.0 # Set to zero if there are no elements\n",
        "\n",
        "\n",
        "    return cumulative_loss/samples, cumulative_accuracy\n",
        "\n",
        "def test_step(net, data_loader, cost_function, device='cuda'):\n",
        "    samples = 0.0\n",
        "    cumulative_loss = 0.0\n",
        "    cumulative_accuracy = 0.0\n",
        "    batch_ious = []\n",
        "\n",
        "    net = net.float()\n",
        "    # set the network to evaluation mode\n",
        "    net.eval()\n",
        "\n",
        "    # disable gradient computation (we are only testing, we do not want our model to be modified in this step!)\n",
        "    with torch.no_grad():\n",
        "        # iterate over the test set\n",
        "\n",
        "      for batch_idx, batch in enumerate(data_loader):\n",
        "\n",
        "        #load data into GPU\n",
        "        batch['image'] = batch['image'].to(device)\n",
        "        batch['captions'] = batch['captions'].to(device)\n",
        "        target_bbox = batch['bbox'].to(device)\n",
        "\n",
        "        # forward pass\n",
        "        out_bbox = net(batch['image'], batch['captions'])\n",
        "\n",
        "        # loss computation\n",
        "        loss = cost_function(out_bbox, target_bbox)\n",
        "        # fetch prediction and loss value\n",
        "\n",
        "        samples += batch['bbox'].shape[0]\n",
        "        cumulative_loss += loss.item() # Note: the .item() is needed to extract scalars from tensors\n",
        "\n",
        "\n",
        "        #iou_accuracy = get_iou_torch(target_bbox, out_bbox)\n",
        "        iou_accuracy = torchvision.ops.box_iou(target_bbox, out_bbox).detach().cpu().numpy().sum().item()\n",
        "        #print(iou.detach().cpu().numpy())\n",
        "        #print(print(iou.detach().cpu().numpy().sum().item()))\n",
        "        #iou_accuracy = iou_pytorch(out_bbox, target_bbox)\n",
        "        #batch_ious.append(iou_accuracy.sum().item())\n",
        "        batch_ious.append(iou_accuracy)\n",
        "        #cumulative_accuracy += iou_accuracy\n",
        "\n",
        "\n",
        "    if batch_ious:\n",
        "        # Calculate the average IoU across all batches\n",
        "        cumulative_accuracy = np.mean(batch_ious)\n",
        "    else:\n",
        "        cumulative_accuracy = 0.0  # Set to zero if there are no elements\n",
        "\n",
        "    return cumulative_loss/samples, cumulative_accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "t2f4f07KFLoF"
      },
      "outputs": [],
      "source": [
        "def _make_grid_(input):\n",
        "    # Get the width and height of the output feature map\n",
        "    _, height, width = input.size()\n",
        "\n",
        "    # Determine the size of each grid cell\n",
        "    grid_size = height // 4\n",
        "    grid_cells = []\n",
        "    boxes = torch.zeros(4, 4, 4)\n",
        "    for i in range(4):\n",
        "        for j in range(4):\n",
        "            x1 = i * grid_size\n",
        "            y1 = j * grid_size\n",
        "            x2 = x1 + grid_size\n",
        "            y2 = y1 + grid_size\n",
        "            boxes[i, j, :] = torch.tensor([float(x1), float(y1), float(x2), float(y2)])\n",
        "            grid_cell = input[:, x1:x2, y1:y2]\n",
        "            grid_cells.append(grid_cell)\n",
        "    grid_cells = torch.stack(grid_cells)\n",
        "    return grid_cells, boxes\n",
        "\n",
        "\n",
        "def _make_grid(input):\n",
        "    # Get the width and height of the output feature map\n",
        "    _, width, height = input.size()\n",
        "    # Determine the size of each grid cell\n",
        "    grid_size = height // 4\n",
        "    grid_cells = []\n",
        "    grid_coordinates = []\n",
        "    for i in range(4):\n",
        "        for j in range(4):\n",
        "            # Calculate the coordinates for the current grid cell\n",
        "            x1 = i * grid_size\n",
        "            y1 = j * grid_size\n",
        "            x2 = x1 + grid_size\n",
        "            y2 = y1 + grid_size\n",
        "            # Extract the region corresponding to the grid cell\n",
        "            grid_cell = input[:, x1:x2, y1:y2]\n",
        "            grid_coordinates.append(torch.tensor([float(x1),float(y1),float(x2),float(y2)]))\n",
        "            grid_cells.append(grid_cell)\n",
        "    grid_cells = torch.stack(grid_cells)\n",
        "    grid_coordinates = torch.stack(grid_coordinates)\n",
        "    return grid_cells, grid_coordinates\n",
        "\n",
        "\n",
        "class Grid(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Grid, self).__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        res = []\n",
        "        for img in x:\n",
        "            res.append(_make_grid(img))\n",
        "        return res\n",
        "\n",
        "\n",
        "class CustomCLIP(torch.nn.Module):\n",
        "  def __init__(self, num_classes: int = 10, bias=False):\n",
        "    super().__init__()\n",
        "    in_features = 1024\n",
        "    out_features = 1024\n",
        "    #self.model = model.children()[:3]\n",
        "    #layers = list(model.children())[:4]\n",
        "    #self.features1 = torch.nn.Sequential(*layers[:2])\n",
        "    #self.features2 = torch.nn.Sequential(*layers[2:])\n",
        "\n",
        "    self.encoder = model.visual.float()\n",
        "    self.conv1 = self.encoder.conv1\n",
        "    self.bn1 = self.encoder.bn1\n",
        "    self.relu1 = self.encoder.relu1\n",
        "    self.avgpool = self.encoder.avgpool\n",
        "    #self.mlp = self.encoder.mlp\n",
        "    self.dropout = torch.nn.Dropout(0.1)\n",
        "    self.dtype = self.encoder.type(self.conv1.weight.dtype)\n",
        "    self.layer1 = self.encoder.layer1\n",
        "    self.attnpool = self.encoder.attnpool\n",
        "\n",
        "\n",
        "    self.text_encoder = self.encode_text\n",
        "\n",
        "    self.grid = Grid()\n",
        "\n",
        "    # add a bottleneck\n",
        "    self.image_encoder = torch.nn.Sequential(\n",
        "      self.conv1,\n",
        "      self.bn1,\n",
        "      self.relu1,\n",
        "      torch.nn.Dropout(0.1),\n",
        "      self.avgpool,\n",
        "      torch.nn.Flatten(),\n",
        "      torch.nn.Linear(20000, num_classes, bias=bias),\n",
        "      torch.nn.Dropout(0.1),\n",
        "    )\n",
        "\n",
        "    '''self.classifier = torch.nn.Sequential(\n",
        "      torch.nn.Linear(25, num_classes, bias=bias),\n",
        "      torch.nn.Dropout(0.1),\n",
        "      torch.nn.BatchNorm2d(32),\n",
        "    )\n",
        "\n",
        "    self.text_classifier = torch.nn.Sequential(\n",
        "      torch.nn.Linear(512, num_classes, bias=bias),\n",
        "      torch.nn.Dropout(0.1),\n",
        "    )'''\n",
        "\n",
        "  def encode_text(self, text):\n",
        "      x = model.token_embedding(text)\n",
        "      #x = x + model.positional_embedding\n",
        "      x = model.ln_final(x)\n",
        "\n",
        "      return x\n",
        "\n",
        "  def forward(self, img, cap):\n",
        "    grids = self.grid(img)\n",
        "    #_, height, width = img.size()\n",
        "    aspect_ratio = 224 / 224\n",
        "    target_width = 100\n",
        "    target_height = int(target_width / aspect_ratio)\n",
        "    boxes = grids[0][1]\n",
        "    boxes = boxes.to(device)\n",
        "    similar = []\n",
        "    for idx, (gd, _) in enumerate(grids):\n",
        "      x_ = []\n",
        "      for g in gd:\n",
        "\n",
        "          x = F.interpolate(g.unsqueeze(0), size=(target_width, target_height), mode='bilinear', align_corners=False)\n",
        "          with torch.no_grad():\n",
        "            x = self.image_encoder(x)\n",
        "          #x = self.classifier(x)\n",
        "          x /= x.norm(dim=-1, keepdim=True)\n",
        "          x_.append(x)\n",
        "\n",
        "      with torch.no_grad():\n",
        "        y = self.text_encoder(cap[idx].unsqueeze(0))\n",
        "        #y = self.text_classifier(y)\n",
        "      y /= y.norm(dim=-1, keepdim=True)\n",
        "\n",
        "\n",
        "      im_ = torch.cat(x_)\n",
        "\n",
        "      #print(im_.shape)\n",
        "      #print(y.shape)\n",
        "      #im_ = im_.squeeze()\n",
        "\n",
        "      #target_len = y.size(1) - im_.size(1)\n",
        "      #im_ = F.pad(im_, (0,0,0,0,0,target_len,0,0), value=0)\n",
        "      #target_len = y.size(3) - im_.size(3)\n",
        "      #im_ = F.pad(im_, (0,target_len,0,0,0,0,0,0), value=0)\n",
        "      target_len = y.size(3) - im_.size(1)\n",
        "      im_ = F.pad(im_, (0,target_len,0,0), value=0)\n",
        "      #print(im_.shape)\n",
        "      #print(y.shape)\n",
        "\n",
        "      similarity = F.softmax(im_ @ y.mT, dim=0)\n",
        "      #similarity = F.softmax(x_ @ y.mT, dim=-1)\n",
        "      top_values, top_indices = similarity.topk(1, dim=0)\n",
        "      #print(boxes[top_indices].shape)\n",
        "      similar.append(boxes[top_indices])\n",
        "    out_bbox = torch.cat(similar)\n",
        "    out_bbox = out_bbox[:, 0, 0, 0, :]\n",
        "    #print(out_bbox.shape)\n",
        "\n",
        "    return out_bbox"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "MGHhKD_Tgpjh"
      },
      "outputs": [],
      "source": [
        "def pad_sequence(batch):\n",
        "  return torch.nn.utils.rnn.pad_sequence(batch, batch_first=True, padding_value=0)\n",
        "def my_collate_fn(batch):\n",
        "\n",
        "  return {\n",
        "      'image': pad_sequence([x['image'] for x in batch]),\n",
        "      'captions': pad_sequence([x['captions'] for x in batch]),\n",
        "      'bbox': pad_sequence([x['bbox'] for x in batch])\n",
        "      }\n",
        "\n",
        "\n",
        "def get_data(data_dir, batch_size, test_batch_size, transform=True, target_transform=False):\n",
        "\n",
        "\n",
        "    if transform:\n",
        "        # convert the PIL images to Tensors\n",
        "        transform = T.Compose([\n",
        "            T.Resize((224, 224)),\n",
        "            T.RandomCrop(size=224),\n",
        "            T.RandomHorizontalFlip(),\n",
        "            T.RandomRotation(degrees=30),\n",
        "            T.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),\n",
        "            T.ToTensor(),\n",
        "            T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "        ])\n",
        "    else:\n",
        "          # prepare data transformations and then combine them sequentially\n",
        "        transform = preprocess\n",
        "    if target_transform:\n",
        "        target_transform = T.Compose([\n",
        "                                 lambda y: torch.zeros(10, dtype=torch.float).scatter_(0, torch.tensor(y), value=1)])\n",
        "    else:\n",
        "        target_transform = None\n",
        "\n",
        "  # load data\n",
        "    train_dataset, test_dataset = preProcess_datasets(data_dir)\n",
        "    #print(len(train_dataset))\n",
        "    #print(len(test_dataset))\n",
        "    full_training_data = RefCOCOgDataset(dataset=train_dataset, transform=transform, target_transform=target_transform, data_dir=data_dir)\n",
        "\n",
        "    #test_dataset = preProcess_datasets(data_dir, train=False)\n",
        "    global test_data\n",
        "    test_data = RefCOCOgDataset(dataset=test_dataset, transform=transform, target_transform=target_transform, data_dir=data_dir)\n",
        "\n",
        "\n",
        "    #evens = list(range(0, len(full_training_data), 2))\n",
        "    #training_data2 = torch.utils.data.Subset(full_training_data, evens)\n",
        "    #training_data2 = torch.utils.data.Subset(full_training_data, range(5000))\n",
        "    #test_data2 = torch.utils.data.Subset(test_data, range(5000))\n",
        "\n",
        "  # create train and validation splits\n",
        "    #num_samples = len(training_data2)\n",
        "    num_samples = len(full_training_data)\n",
        "\n",
        "    training_samples = int(num_samples * 0.8 + 1)\n",
        "    validation_samples = num_samples - training_samples\n",
        "\n",
        "    #training_data, validation_data = torch.utils.data.random_split(training_data2, [training_samples, validation_samples])\n",
        "    training_data, validation_data = torch.utils.data.random_split(full_training_data, [training_samples, validation_samples])\n",
        "    print(len(training_data))\n",
        "\n",
        "    print(len(validation_data))\n",
        "    print(len(test_data))\n",
        "  # initialize dataloaders_collate\n",
        "    train_loader = torch.utils.data.DataLoader(training_data, batch_size, shuffle=True, num_workers=2, collate_fn=my_collate_fn)\n",
        "    val_loader = torch.utils.data.DataLoader(validation_data, test_batch_size, shuffle=False, num_workers=2, collate_fn=my_collate_fn)\n",
        "    test_loader = torch.utils.data.DataLoader(test_data, test_batch_size, shuffle=False, num_workers=2, collate_fn=my_collate_fn)\n",
        "    #test_loader = torch.utils.data.DataLoader(test_data2, test_batch_size, shuffle=False, num_workers=2, collate_fn=my_collate_fn)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # pre-train model on zero-shot transfer learning\n",
        "    #test_accuracy = test_step_zero_shot_clip(model, test_loader, texts_z)\n",
        "\n",
        "\n",
        "    # evaluate accuracy on zero-shot learning\n",
        "\n",
        "    #print(\"Test accuracy {:.2f}\".format(test_accuracy))\n",
        "\n",
        "    return train_loader, val_loader, test_loader\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "V-TbnepNN8_u"
      },
      "outputs": [],
      "source": [
        "# tensorboard logging utilities\n",
        "def log_values(writer, step, loss, accuracy, prefix):\n",
        "  writer.add_scalar(f\"{prefix}/loss\", loss, step)\n",
        "  writer.add_scalar(f\"{prefix}/accuracy\", accuracy, step)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "k6UDZHcZksAv"
      },
      "outputs": [],
      "source": [
        "model, preprocess = clip.load(\"RN50\")\n",
        "model = model.float()\n",
        "modified_model = CustomCLIP(num_classes=10).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "RmcsfGIf_1fd"
      },
      "outputs": [],
      "source": [
        "# main funcition\n",
        "def main(\n",
        "      root='/content/dataset/refcocog/',\n",
        "      data_dir='dataset/refcocog',\n",
        "      batch_size=64,\n",
        "      test_batch_size=64,\n",
        "      num_classes=10,\n",
        "      learning_rate=0.01,\n",
        "      weight_decay=1e-5,\n",
        "      momentum=0.9,\n",
        "      epochs=10\n",
        "      #epochs=3\n",
        "    ):\n",
        "  writer = SummaryWriter(log_dir=\"runs/exp1\")\n",
        "  #global test_loader\n",
        "  # train clip on zero-shot learning and instantiates dataloaders\n",
        "  train_loader, val_loader, test_loader = get_data(data_dir=data_dir, batch_size=batch_size, test_batch_size=test_batch_size, transform=True, target_transform=True)\n",
        "\n",
        "\n",
        "  # instantiate the network and move it to the chosen device (GPU)\n",
        "\n",
        "\n",
        "  # instantiate the optimizer\n",
        "  optimizer = get_optimizer(modified_model, learning_rate, weight_decay, momentum)\n",
        "\n",
        "  # define the cost function\n",
        "  cost_function = get_cost_function()\n",
        "\n",
        "  # evaluate accuracy of modified model on zero-shot learning\n",
        "  #test_accuracy = test_step_zero_shot_clip(model, test_loader, texts_z).to(device)\n",
        "\n",
        "  #print(cosine_similarity(images_z, texts_z))\n",
        "  #print(\"Test accuracy {:.2f}\".format(test_accuracy))\n",
        "\n",
        "  # computes evaluation results before training\n",
        "  print('Before training:')\n",
        "  train_loss, train_accuracy = test_step(modified_model, train_loader, cost_function)\n",
        "  val_loss, val_accuracy = test_step(modified_model, val_loader, cost_function)\n",
        "  test_loss, test_accuracy = test_step(modified_model, test_loader, cost_function)\n",
        "\n",
        "  # log to TensorBoard\n",
        "  log_values(writer, -1, train_loss, train_accuracy, \"train\")\n",
        "  log_values(writer, -1, val_loss, val_accuracy, \"validation\")\n",
        "  log_values(writer, -1, test_loss, test_accuracy, \"test\")\n",
        "\n",
        "  print('\\tTraining loss {:.5f}, Training accuracy {:.2f}'.format(train_loss, train_accuracy))\n",
        "  print('\\tValidation loss {:.5f}, Validation accuracy {:.2f}'.format(val_loss, val_accuracy))\n",
        "  print('\\tTest loss {:.5f}, Test accuracy {:.2f}'.format(test_loss, test_accuracy))\n",
        "  print('-----------------------------------------------------')\n",
        "\n",
        "  # for each epoch, train the network and then compute evaluation results\n",
        "  for e in range(epochs):\n",
        "\n",
        "    train_loss, train_accuracy = training_step(modified_model, train_loader, optimizer, cost_function)\n",
        "    val_loss, val_accuracy = test_step(modified_model, val_loader, cost_function)\n",
        "\n",
        "    # logs to TensorBoard\n",
        "    log_values(writer, e, val_loss, val_accuracy, \"Validation\")\n",
        "\n",
        "    print('Epoch: {:d}'.format(e+1))\n",
        "    print('\\tTraining loss {:.5f}, Training accuracy {:.2f}'.format(train_loss, train_accuracy))\n",
        "    print('\\tValidation loss {:.5f}, Validation accuracy {:.2f}'.format(val_loss, val_accuracy))\n",
        "    print('-----------------------------------------------------')\n",
        "\n",
        "    # compute final evaluation results\n",
        "    print('After training:')\n",
        "    train_loss, train_accuracy = test_step(modified_model, train_loader, cost_function)\n",
        "    val_loss, val_accuracy = test_step(modified_model, val_loader, cost_function)\n",
        "    test_loss, test_accuracy = test_step(modified_model, test_loader, cost_function)\n",
        "\n",
        "    # log to TensorBoard\n",
        "    log_values(writer, epochs, train_loss, train_accuracy, \"train\")\n",
        "    log_values(writer, epochs, val_loss, val_accuracy, \"validation\")\n",
        "    log_values(writer, epochs, test_loss, test_accuracy, \"test\")\n",
        "\n",
        "    print('\\tTraining loss {:.5f}, Training accuracy {:.2f}'.format(train_loss, train_accuracy))\n",
        "    print('\\tValidation loss {:.5f}, Validation accuracy {:.2f}'.format(val_loss, val_accuracy))\n",
        "    print('\\tTest loss {:.5f}, Test accuracy {:.2f}'.format(test_loss, test_accuracy))\n",
        "    print('-----------------------------------------------------')\n",
        "\n",
        "\n",
        "  # closes the logger\n",
        "  writer.close()\n",
        "  torch.save(modified_model.state_dict(), 'model.pth')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "7IXV0biKmV1u"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:<1024>\"\n",
        "os.environ[\"CUDA_LAUNCH_BLOCKING\"]=\"1\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "A_S2MFs8VJER",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc4a0cd5-29cc-4611-cd68-0eeef590d884"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.2.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install scikit-learn\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import svm, datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "def calculate_accuracy(output, target):\n",
        "\n",
        "\n",
        "  f1_scr = f1_score(target, output, average='weighted')\n",
        "\n",
        "  cm = confusion_matrix(target, output)\n",
        "  TP = cm[0][0]\n",
        "  FP = cm[0][1]\n",
        "  FN = cm[1][0]\n",
        "  TN = cm[1][1]\n",
        "\n",
        "  #accuracy = np.sum(np.diagonal(cm)) / np.sum(cm)\n",
        "  #accuracy = (TP+TN)/(TP+FP+FN+TN)\n",
        "  return f1_scr"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#torch.cuda.empty_cache()\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:<1024>\"\n",
        "os.environ[\"CUDA_LAUNCH_BLOCKING\"]=\"1\"\n",
        "os.environ[\"TORCH_USE_CUDA_DSA\"]=\"1\""
      ],
      "metadata": {
        "id": "RFDOfbwYplhK"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CdKpBW-vPQfr",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d1583e1-2ccc-4ea5-8a59-95443e42f5ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4001\n",
            "999\n",
            "5023\n",
            "Before training:\n",
            "\tTraining loss 1.39093, Training accuracy 94.81\n",
            "\tValidation loss 1.41712, Validation accuracy 87.07\n",
            "\tTest loss 1.39667, Test accuracy 80.69\n",
            "-----------------------------------------------------\n",
            "Epoch: 1\n",
            "\tTraining loss 1.39038, Training accuracy 94.49\n",
            "\tValidation loss 1.41712, Validation accuracy 87.07\n",
            "-----------------------------------------------------\n",
            "After training:\n",
            "\tTraining loss 1.39072, Training accuracy 94.90\n",
            "\tValidation loss 1.41712, Validation accuracy 87.07\n",
            "\tTest loss 1.39667, Test accuracy 80.69\n",
            "-----------------------------------------------------\n",
            "Epoch: 2\n",
            "\tTraining loss 1.39034, Training accuracy 94.99\n",
            "\tValidation loss 1.41712, Validation accuracy 87.07\n",
            "-----------------------------------------------------\n",
            "After training:\n",
            "\tTraining loss 1.39144, Training accuracy 95.12\n",
            "\tValidation loss 1.41712, Validation accuracy 87.07\n",
            "\tTest loss 1.39667, Test accuracy 80.69\n",
            "-----------------------------------------------------\n",
            "Epoch: 3\n",
            "\tTraining loss 1.39034, Training accuracy 94.94\n",
            "\tValidation loss 1.41712, Validation accuracy 87.07\n",
            "-----------------------------------------------------\n",
            "After training:\n",
            "\tTraining loss 1.39047, Training accuracy 94.56\n",
            "\tValidation loss 1.41712, Validation accuracy 87.07\n",
            "\tTest loss 1.39667, Test accuracy 80.69\n",
            "-----------------------------------------------------\n",
            "Epoch: 4\n",
            "\tTraining loss 1.39131, Training accuracy 94.74\n",
            "\tValidation loss 1.41712, Validation accuracy 87.07\n",
            "-----------------------------------------------------\n",
            "After training:\n",
            "\tTraining loss 1.39121, Training accuracy 94.60\n",
            "\tValidation loss 1.41712, Validation accuracy 87.07\n",
            "\tTest loss 1.39667, Test accuracy 80.69\n",
            "-----------------------------------------------------\n",
            "Epoch: 5\n",
            "\tTraining loss 1.38983, Training accuracy 94.76\n",
            "\tValidation loss 1.41712, Validation accuracy 87.07\n",
            "-----------------------------------------------------\n",
            "After training:\n",
            "\tTraining loss 1.39171, Training accuracy 95.06\n",
            "\tValidation loss 1.41712, Validation accuracy 87.07\n",
            "\tTest loss 1.39667, Test accuracy 80.69\n",
            "-----------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CTW32KCDQvVJ"
      },
      "outputs": [],
      "source": [
        "def draw_corner_rect(bb, color='red'):\n",
        "    bb = np.array(bb, dtype=np.float32)\n",
        "    print(bb)\n",
        "    return plt.Rectangle((bb[1], bb[0]), bb[3]-bb[1], bb[2]-bb[0], color=color,\n",
        "                         fill=False, lw=3)\n",
        "\n",
        "def draw_bbx(bb, color):\n",
        "    plt.gca().add_patch(draw_corner_rect(bb, color))\n",
        "\n",
        "\n",
        "def inference(model, query, image, gd_bbox):\n",
        "\n",
        "    model.load_state_dict(torch.load('model.pth'))\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "      bbox = model(image.unsqueeze(0).to(device), query.unsqueeze(0).to(device))\n",
        "      print('inference')\n",
        "      print(bbox)\n",
        "      print(bbox.shape)\n",
        "\n",
        "      #caption = tokenizer.decode(query[0], skip_special_tokens=False)\n",
        "\n",
        "\n",
        "      image = image.permute(1,2,0)\n",
        "      img = cv2.cvtColor(image.numpy(), cv2.COLOR_BGR2RGB)\n",
        "\n",
        "      out_bb = bbox.detach().cpu().numpy()\n",
        "      out_bb = out_bb.astype(int)\n",
        "      print(out_bb)\n",
        "      print(out_bb.shape)\n",
        "      plt.imshow(img)\n",
        "\n",
        "      draw_bbx(out_bb[0],'red')\n",
        "\n",
        "      plt.axis('off')\n",
        "      plt.show()\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JzgeqEvWmnfv"
      },
      "outputs": [],
      "source": [
        "x = test_data[4]\n",
        "\n",
        "\n",
        "inference(modified_model,\n",
        "             query=x['captions'],\n",
        "             image=x['image'],\n",
        "              gd_bbox=x['bbox'])\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
