{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0SCUZq8R27Nv",
        "outputId": "fae53ac0-4119-4e9a-b05a-a75ebd9ec96c",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "#from google.colab import drive\n",
        "\n",
        "#drive.mount('/content/drive')\n",
        "!mkdir dataset\n",
        "!pip install gdown\n",
        "!gdown 1xijq32XfEm6FPhUb7RsZYWHc2UuwVkiq\n",
        "!mv refcocog.tar.gz ./dataset/\n",
        "#!curl -L -o refcocog.tar.g \"https://drive.google.com/uc?id=1xijq32XfEm6FPhUb7RsZYWHc2UuwVkiq\"\n",
        "#!mv refcocog.tar.gz ./dataset/\n",
        "#!sudo install curl\n",
        "#import tarfile\n",
        "#data_dir='/content/drive/MyDrive/refcocog.tar.gz'\n",
        "# Extract data\n",
        "#tar = tarfile.open(data_dir)\n",
        "#tar.extractall('dataset/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Bum1NKU2v5a",
        "outputId": "9e250df8-4503-433d-a02e-2c66043db997"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tar: dataset/refcocog.tar.gz: Cannot open: No such file or directory\n",
            "tar: Error is not recoverable: exiting now\n",
            "refcocog\n"
          ]
        }
      ],
      "source": [
        "!tar -xf dataset/refcocog.tar.gz -C dataset\n",
        "!ls dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LZum46_RLQ44",
        "outputId": "0aa6282b-b276-4d54-8b56-2d0c9903f4a1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/openai/CLIP.git\n",
            "  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-upwb3glr\n",
            "  Running command git clone -q https://github.com/openai/CLIP.git /tmp/pip-req-build-upwb3glr\n",
            "Requirement already satisfied (use --upgrade to upgrade): clip==1.0 from git+https://github.com/openai/CLIP.git in /home/disi/.local/lib/python3.8/site-packages\n",
            "Requirement already satisfied: ftfy in /home/disi/.local/lib/python3.8/site-packages (from clip==1.0) (6.1.3)\n",
            "Requirement already satisfied: regex in /home/disi/.local/lib/python3.8/site-packages (from clip==1.0) (2023.12.25)\n",
            "Requirement already satisfied: torch in /home/disi/.local/lib/python3.8/site-packages (from clip==1.0) (2.1.2)\n",
            "Requirement already satisfied: torchvision in /home/disi/.local/lib/python3.8/site-packages (from clip==1.0) (0.16.2)\n",
            "Requirement already satisfied: tqdm in /home/disi/.local/lib/python3.8/site-packages (from clip==1.0) (4.66.1)\n",
            "Requirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /home/disi/.local/lib/python3.8/site-packages (from ftfy->clip==1.0) (0.2.13)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/disi/.local/lib/python3.8/site-packages (from torch->clip==1.0) (12.1.0.106)\n",
            "Requirement already satisfied: typing-extensions in /home/disi/.local/lib/python3.8/site-packages (from torch->clip==1.0) (4.9.0)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.18.1; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/disi/.local/lib/python3.8/site-packages (from torch->clip==1.0) (2.18.1)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/disi/.local/lib/python3.8/site-packages (from torch->clip==1.0) (10.3.2.106)\n",
            "Requirement already satisfied: jinja2 in /usr/lib/python3/dist-packages (from torch->clip==1.0) (2.10.1)\n",
            "Requirement already satisfied: networkx in /home/disi/.local/lib/python3.8/site-packages (from torch->clip==1.0) (3.1)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/disi/.local/lib/python3.8/site-packages (from torch->clip==1.0) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/disi/.local/lib/python3.8/site-packages (from torch->clip==1.0) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/disi/.local/lib/python3.8/site-packages (from torch->clip==1.0) (12.1.105)\n",
            "Requirement already satisfied: triton==2.1.0; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/disi/.local/lib/python3.8/site-packages (from torch->clip==1.0) (2.1.0)\n",
            "Requirement already satisfied: fsspec in /home/disi/.local/lib/python3.8/site-packages (from torch->clip==1.0) (2023.12.2)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/disi/.local/lib/python3.8/site-packages (from torch->clip==1.0) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/disi/.local/lib/python3.8/site-packages (from torch->clip==1.0) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/disi/.local/lib/python3.8/site-packages (from torch->clip==1.0) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/disi/.local/lib/python3.8/site-packages (from torch->clip==1.0) (12.1.105)\n",
            "Requirement already satisfied: filelock in /home/disi/.local/lib/python3.8/site-packages (from torch->clip==1.0) (3.13.1)\n",
            "Requirement already satisfied: sympy in /home/disi/.local/lib/python3.8/site-packages (from torch->clip==1.0) (1.12)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/disi/.local/lib/python3.8/site-packages (from torch->clip==1.0) (8.9.2.26)\n",
            "Requirement already satisfied: numpy in /home/disi/.local/lib/python3.8/site-packages (from torchvision->clip==1.0) (1.24.4)\n",
            "Requirement already satisfied: requests in /usr/lib/python3/dist-packages (from torchvision->clip==1.0) (2.22.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/disi/.local/lib/python3.8/site-packages (from torchvision->clip==1.0) (10.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/disi/.local/lib/python3.8/site-packages (from nvidia-cusparse-cu12==12.1.0.106; platform_system == \"Linux\" and platform_machine == \"x86_64\"->torch->clip==1.0) (12.3.101)\n",
            "Requirement already satisfied: mpmath>=0.19 in /home/disi/.local/lib/python3.8/site-packages (from sympy->torch->clip==1.0) (1.3.0)\n",
            "Building wheels for collected packages: clip\n",
            "  Building wheel for clip (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369522 sha256=8a4cb9b678f4846664db6b7164da91970814c043346c94d9b98466d85ba8892c\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-ac37xh0a/wheels/ab/4f/3a/5e51521b55997aa6f0690e095c08824219753128ce8d9969a3\n",
            "Successfully built clip\n"
          ]
        }
      ],
      "source": [
        "!pip install git+https://github.com/openai/CLIP.git\n",
        "from clip import clip\n",
        "\n",
        "model, preprocess = clip.load(\"RN50\")\n",
        "\n",
        "model = model.float()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Cxpf1l-vxeg"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:<1024>\"\n",
        "os.environ[\"CUDA_LAUNCH_BLOCKING\"]=\"1\"\n",
        "os.environ[\"TORCH_USE_CUDA_DSA\"]=\"1\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LdnLcbFyfkvW"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as T\n",
        "from torchvision import ops\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "from posixpath import split\n",
        "import json\n",
        "import tarfile\n",
        "import io\n",
        "import pickle\n",
        "from PIL import Image\n",
        "import re\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "from itertools import chain\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# import some common libraries\n",
        "import cv2\n",
        "#from google.colab.patches import cv2_imshow\n",
        "# Check if CUDA is available\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "data = {}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PxhqoAtfkLaI"
      },
      "outputs": [],
      "source": [
        "def preProcess_datasets(data_dir):\n",
        "  train_dataset = {}\n",
        "  test_dataset = {}\n",
        "  f = open(f'{data_dir}/annotations/refs(umd).p', 'rb')\n",
        "  data['refs'] = pickle.load(f)\n",
        "  instances_file = os.path.join(f'{data_dir}/annotations/instances.json')\n",
        "  instances = json.load(open(instances_file, 'r'))\n",
        "  data['images'] = instances['images']\n",
        "  data['annotations'] = instances['annotations']\n",
        "\n",
        "  c=0\n",
        "  d=0\n",
        "  for key,val in enumerate(data['refs']):\n",
        "    for v in data['images']:\n",
        "      if val['image_id'] == v['id']:\n",
        "        fname = os.path.join(data_dir+'/images/', v['file_name'])\n",
        "        if os.path.exists(fname):\n",
        "          if val['split'] == 'train' or val['split'] == 'val':\n",
        "            train_dataset[c] = {}\n",
        "            train_dataset[c]['image'] = v['file_name']\n",
        "            train_dataset[c]['captions'] = [t['sent'] for t in val['sentences']]\n",
        "            train_dataset[c]['id'] = val['image_id']\n",
        "            break\n",
        "          else:\n",
        "            if d>=5000:\n",
        "                break\n",
        "            test_dataset[d] = {}\n",
        "            test_dataset[d]['image'] = v['file_name']\n",
        "            test_dataset[d]['captions'] = [t['sent'] for t in val['sentences']]\n",
        "            test_dataset[d]['id'] = val['image_id']\n",
        "            break\n",
        "    for ann in data['annotations']:\n",
        "      if val['ann_id'] == ann['id']:\n",
        "        if val['split'] == 'train' or val['split'] == 'val':\n",
        "          train_dataset[c]['bbox'] = ann['bbox']\n",
        "          c+=1\n",
        "          break\n",
        "        else:\n",
        "          if d>=5000:\n",
        "            break\n",
        "          test_dataset[d]['bbox'] = ann['bbox']\n",
        "          d+=1\n",
        "          break\n",
        "    if c>=5000:\n",
        "      break\n",
        "\n",
        "  return train_dataset, test_dataset\n",
        "\n",
        "\n",
        "def resize_bounding_box(bbox, original_size, new_size):\n",
        "  x_min, y_min, x_max, y_max = bbox\n",
        "  orig_width, orig_height = original_size\n",
        "  new_width, new_height = new_size\n",
        "\n",
        "  # Calculate the scaling factors\n",
        "  width_ratio = new_width / orig_width\n",
        "  height_ratio = new_height / orig_height\n",
        "\n",
        "  # Resize the bounding box coordinates\n",
        "  new_x_min = int(x_min * width_ratio)\n",
        "  new_y_min = int(y_min * height_ratio)\n",
        "  new_x_max = int(x_max * width_ratio)\n",
        "  new_y_max = int(y_max * height_ratio)\n",
        "\n",
        "  return new_x_min, new_y_min, new_x_max, new_y_max\n",
        "\n",
        "def xywh_to_xyxy(boxes):\n",
        "  xmin = boxes[0]\n",
        "  ymin = boxes[1]\n",
        "  w = boxes[2]\n",
        "  h = boxes[3]\n",
        "  xmax = xmin + w\n",
        "  ymax = ymin + h\n",
        "  return [xmin, ymin, xmax, ymax]\n",
        "\n",
        "\n",
        "def normalize_text(text):\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove punctuation\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "\n",
        "    # Remove extra whitespaces\n",
        "    text = ' '.join(text.split())\n",
        "\n",
        "    #remove special characters\n",
        "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
        "\n",
        "    #replace numbers\n",
        "    text = re.sub(r'\\d+', \"[NUM]\", text)\n",
        "\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2b4mMLT8vbdE"
      },
      "outputs": [],
      "source": [
        "class RefCOCOgDataset(Dataset):\n",
        "    def __init__(self, dataset, transform, target_transform, data_dir='dataset/refcocog'):\n",
        "        super(RefCOCOgDataset, self).__init__()\n",
        "        self.data_dir = data_dir\n",
        "        self.transform = transform\n",
        "        self.target_transform = target_transform\n",
        "        self.dataset = dataset\n",
        "\n",
        "\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        data_item = {}\n",
        "        fname = os.path.join(self.data_dir+'/images/', self.dataset[idx]['image'])\n",
        "        image = Image.open(fname).convert('RGB')\n",
        "        shape_0 = list(image.size)\n",
        "        if self.transform:\n",
        "            data_item['image'] = self.transform(image).float()\n",
        "        if self.target_transform:\n",
        "            data_item['captions'] = clip.tokenize([self.target_transform(sent) for sent in self.dataset[idx]['captions']]).float()\n",
        "            bb = xywh_to_xyxy(self.dataset[idx]['bbox'])\n",
        "            bbox = resize_bounding_box(bb, shape_0, [224,224])\n",
        "            data_item['bbox'] = torch.FloatTensor(bbox)\n",
        "        return data_item\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t2f4f07KFLoF"
      },
      "outputs": [],
      "source": [
        "def _make_grid(input):\n",
        "    # Get the width and height of the output feature map\n",
        "    _, width, height = input.size()\n",
        "    # Determine the size of each grid cell\n",
        "    grid_size = height // 16\n",
        "    grid_cells = []\n",
        "    grid_coordinates = []\n",
        "    for i in range(16):\n",
        "        for j in range(16):\n",
        "            # Calculate the coordinates for the current grid cell\n",
        "            x1 = i * grid_size\n",
        "            y1 = j * grid_size\n",
        "            x2 = x1 + grid_size\n",
        "            y2 = y1 + grid_size\n",
        "            # Extract the region corresponding to the grid cell\n",
        "            grid_cell = input[ :, x1:x2, y1:y2]\n",
        "            grid_coordinates.append(torch.FloatTensor([float(x1),float(y1),float(x2),float(y2)]))\n",
        "            grid_cells.append(grid_cell)\n",
        "    grid_cells = torch.stack(grid_cells)\n",
        "    grid_coordinates = torch.stack(grid_coordinates)\n",
        "    return grid_cells, grid_coordinates\n",
        "\n",
        "\n",
        "class Grid(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Grid, self).__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        res = []\n",
        "        for img in x:\n",
        "            res.append(_make_grid(img))\n",
        "        return res\n",
        "\n",
        "\n",
        "class CustomCLIP(torch.nn.Module):\n",
        "  def __init__(self, num_classes: int = 4, bias=True):\n",
        "    super(CustomCLIP, self).__init__()\n",
        "    self.num_classes = num_classes\n",
        "    self.conv = model.visual.float().conv1\n",
        "    self.bn = model.visual.float().bn1\n",
        "    self.relu = model.visual.float().relu1\n",
        "    self.pool = model.visual.float().avgpool\n",
        "    self.fc1 = torch.nn.Linear(288, self.num_classes, bias=bias)\n",
        "    self.fc2 = torch.nn.Linear(77, self.num_classes, bias=bias)\n",
        "    self.flttn = torch.nn.Flatten()\n",
        "    self.tok_emb = model.token_embedding\n",
        "    self.dropout = torch.nn.Dropout(0.1)\n",
        "\n",
        "    torch.nn.init.kaiming_uniform_(self.conv.weight, a=0, mode='fan_in', nonlinearity='leaky_relu')\n",
        "    #torch.nn.init.zeros_(self.conv.bias)\n",
        "\n",
        "    # Initialize parameters for batch normalization layer\n",
        "    torch.nn.init.ones_(self.bn.weight)\n",
        "    torch.nn.init.zeros_(self.bn.bias)\n",
        "\n",
        "\n",
        "    # Initialize parameters for the linear layer\n",
        "    torch.nn.init.xavier_uniform_(self.fc1.weight)\n",
        "    torch.nn.init.zeros_(self.fc1.bias)\n",
        "\n",
        "\n",
        "    # Initialize parameters for the linear layer\n",
        "    torch.nn.init.xavier_uniform_(self.fc2.weight)\n",
        "    torch.nn.init.zeros_(self.fc2.bias)\n",
        "\n",
        "    self.text_encoder = torch.nn.Sequential(\n",
        "      #self.tok_emb,\n",
        "      self.fc2,\n",
        "      self.dropout,\n",
        "    )\n",
        "\n",
        "\n",
        "    self.grid = Grid()\n",
        "\n",
        "    # add a bottleneck\n",
        "    self.image_encoder = torch.nn.Sequential(\n",
        "      self.conv,\n",
        "      self.bn,\n",
        "      self.relu,\n",
        "      self.dropout,\n",
        "      self.pool,\n",
        "      self.flttn,\n",
        "      self.fc1,\n",
        "      self.dropout,\n",
        "\n",
        "    )\n",
        "\n",
        "\n",
        "  def forward(self, img, cap):\n",
        "    grids = self.grid(img)\n",
        "    aspect_ratio = 224 / 224\n",
        "    target_width = 224 // 16\n",
        "    target_height = int(target_width / aspect_ratio)\n",
        "    similar = []\n",
        "    for idx, (gd, _) in enumerate(grids):\n",
        "      x_ = []\n",
        "      for g in gd:\n",
        "          x = F.interpolate(g.unsqueeze(0), size=(target_width, target_height), mode='bilinear', align_corners=False)\n",
        "          x = self.image_encoder(x)\n",
        "          x = torch.norm(x, dim=-1, keepdim=True)\n",
        "          x_.append(x)\n",
        "      #with torch.no_grad():\n",
        "      #text = cap[idx].unsqueeze(0).to(torch.int32)\n",
        "      y = self.text_encoder(cap[idx].unsqueeze(0))\n",
        "      #y = y[:, :, :, :4]\n",
        "      y = torch.norm(y, dim=-1, keepdim=True)\n",
        "\n",
        "\n",
        "      im_ = torch.cat(x_)\n",
        "\n",
        "      target_len = im_.size(0) - y.size(1)\n",
        "      y = F.pad(im_, (0,target_len,0,0), value=0)\n",
        "\n",
        "\n",
        "      similarity_scores = torch.nn.functional.cosine_similarity(im_, y, dim=0)\n",
        "      max_values, max_indices = torch.max(similarity_scores, dim=0)\n",
        "      boxes = grids[idx][1]\n",
        "      selected_box = boxes[max_indices.to(boxes.device)]\n",
        "\n",
        "      similar.append(selected_box)\n",
        "\n",
        "    out_bbox = torch.cat(similar, dim=0)\n",
        "\n",
        "    out_bbox = out_bbox.view(-1, 4)\n",
        "\n",
        "    return out_bbox\n",
        "\n",
        "  def parameters(self, recurse: bool = True):\n",
        "    return self.get_modified_model_parameters(recurse)\n",
        "\n",
        "  def get_modified_model_parameters(self, recurse: bool = True):\n",
        "    leaf_param_nodes = []\n",
        "    for name, param in self.named_parameters(recurse=recurse):\n",
        "      if param.requires_grad and param.is_leaf:\n",
        "        leaf_param_nodes.append(param)\n",
        "    return leaf_param_nodes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2lv7czaxvxeh"
      },
      "outputs": [],
      "source": [
        "# instantiate the network and move it to the chosen device (GPU)\n",
        "modified_model = CustomCLIP(num_classes=4).float().to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nB6aV1uyGAxd"
      },
      "outputs": [],
      "source": [
        "def get_cost_function():\n",
        "  cost_function = torch.nn.SmoothL1Loss()\n",
        "  return cost_function\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XFYNX6i_GA-O"
      },
      "outputs": [],
      "source": [
        "def get_optimizer(model, lr, wd, momentum, eps):\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=wd, eps=eps)\n",
        "\n",
        "    return optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TIeelz-AF32o"
      },
      "outputs": [],
      "source": [
        "def training_step(net, data_loader, optimizer, cost_function):\n",
        "    with torch.set_grad_enabled(True):\n",
        "        samples = 0.0\n",
        "        cumulative_loss = 0.0\n",
        "        cumulative_accuracy = 0.0\n",
        "        batch_ious = []\n",
        "        # set the network to training mode\n",
        "\n",
        "        net.train()\n",
        "        accumulation_steps = 2\n",
        "        clip_value = 2.0\n",
        "        #scaler = GradScaler()\n",
        "        # iterate over the training set\n",
        "        # Check requires_grad for each parameter\n",
        "\n",
        "        for batch_idx, batch in enumerate(data_loader):\n",
        "            #print(f\"Input Data: {batch['image'].min()}, {batch['image'].max()}, {batch['captions'].max()}, {batch['captions'].max()}\")\n",
        "            # forward pass\n",
        "            #with autocast():\n",
        "                #load data into GPU\n",
        "            batch['image'] = batch['image'].to(device)\n",
        "            batch['captions'] = batch['captions'].to(device)\n",
        "            target_bbox = batch['bbox'].to(device).requires_grad_()\n",
        "            out_bbox = net(batch['image'], batch['captions']).to(device).requires_grad_()\n",
        "\n",
        "            # loss computation\n",
        "            loss = cost_function(out_bbox, target_bbox)\n",
        "            loss = loss / accumulation_steps\n",
        "\n",
        "\n",
        "            if torch.isnan(target_bbox).any() or torch.isnan(out_bbox).any() or torch.isnan(loss).any() or any(torch.isnan(p).any() for p in net.parameters()):\n",
        "                print(\"NaN detected in loss or model parameters. Training stopped.\")\n",
        "                return\n",
        "            if torch.isinf(target_bbox).any() or torch.isinf(out_bbox).any() or torch.isinf(loss).any() or any(torch.isinf(p).any() for p in net.parameters()):\n",
        "                print(\"inf detected in loss or model parameters. Training stopped.\")\n",
        "                return\n",
        "\n",
        "            #backward-pass\n",
        "            loss.backward()\n",
        "\n",
        "\n",
        "            if (batch_idx + 1) % accumulation_steps == 0:\n",
        "                # Gradient scaling and optimization step\n",
        "                torch.nn.utils.clip_grad_norm_(net.parameters(), clip_value) #ommit this\n",
        "                # parameters update\n",
        "                optimizer.step()\n",
        "                # gradients reset\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "            else:\n",
        "                continue\n",
        "\n",
        "\n",
        "            # fetch prediction and loss value\n",
        "            samples += batch['bbox'].shape[0]\n",
        "\n",
        "            cumulative_loss += loss.sum().item()\n",
        "\n",
        "            iou_accuracy = torchvision.ops.box_iou(out_bbox, target_bbox)\n",
        "            acc, _ = torch.max(iou_accuracy, dim=1)\n",
        "            batch_ious.append(acc)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "            # Calculate the average IoU across all batches\n",
        "        if batch_ious:\n",
        "            cumulative_accuracy = torch.cat(batch_ious).mean().item()\n",
        "        else:\n",
        "            cumulative_accuracy = 0.0 # Set to zero if there are no elements\n",
        "\n",
        "        average_loss = cumulative_loss / samples if samples > 0 else 0.0\n",
        "    return average_loss, cumulative_accuracy\n",
        "\n",
        "def test_step(net, data_loader, cost_function):\n",
        "    samples = 0.0\n",
        "    cumulative_loss = 0.0\n",
        "    cumulative_accuracy = 0.0\n",
        "    batch_ious = []\n",
        "    #net = net.float()\n",
        "    # set the network to evaluation mode\n",
        "    net.eval()\n",
        "\n",
        "    # disable gradient computation (we are only testing, we do not want our model to be modified in this step!)\n",
        "    with torch.no_grad():\n",
        "        # iterate over the test set\n",
        "\n",
        "        for batch_idx, batch in enumerate(data_loader):\n",
        "\n",
        "            #load data into GPU\n",
        "            batch['image'] = batch['image'].to(device)\n",
        "            batch['captions'] = batch['captions'].to(device)\n",
        "            target_bbox = batch['bbox'].to(device)\n",
        "\n",
        "            # forward pass\n",
        "            out_bbox = net(batch['image'], batch['captions']).to(device)\n",
        "\n",
        "            # loss computation on GPU\n",
        "            loss = cost_function(out_bbox, target_bbox)\n",
        "\n",
        "            # accumulate the loss on the CPU\n",
        "            cumulative_loss += loss.sum().item()\n",
        "\n",
        "            samples += batch['bbox'].shape[0]\n",
        "\n",
        "            iou_accuracy = torchvision.ops.box_iou(out_bbox, target_bbox)\n",
        "            acc, _ = torch.max(iou_accuracy, dim=1)\n",
        "            batch_ious.append(acc)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "            # Calculate the average IoU across all batches\n",
        "        if batch_ious:\n",
        "            cumulative_accuracy = torch.cat(batch_ious).mean().item()\n",
        "        else:\n",
        "            cumulative_accuracy = 0.0  # Set to zero if there are no elements\n",
        "\n",
        "        average_loss = cumulative_loss / samples if samples > 0 else 0.0\n",
        "    return average_loss, cumulative_accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MGHhKD_Tgpjh"
      },
      "outputs": [],
      "source": [
        "def pad_sequence(batch):\n",
        "  return torch.nn.utils.rnn.pad_sequence(batch, batch_first=True, padding_value=0)\n",
        "def my_collate_fn(batch):\n",
        "\n",
        "  return {\n",
        "      'image': pad_sequence([x['image'] for x in batch]),\n",
        "      'captions': pad_sequence([x['captions'] for x in batch]),\n",
        "      'bbox': pad_sequence([x['bbox'] for x in batch])\n",
        "      }\n",
        "\n",
        "\n",
        "def get_data(data_dir, batch_size, test_batch_size, transform=True, target_transform=True):\n",
        "    global transform_mat\n",
        "    if transform:\n",
        "        # convert the PIL images to Tensors\n",
        "        transform_mat = T.Compose([\n",
        "            T.Resize((224, 224)),\n",
        "            T.RandomCrop(size=224),\n",
        "            T.RandomHorizontalFlip(),\n",
        "            T.RandomRotation(degrees=30),\n",
        "            T.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),\n",
        "            T.ToTensor(),\n",
        "            T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "        ])\n",
        "    else:\n",
        "          # prepare data transformations and then combine them sequentially\n",
        "        transform_mat = None\n",
        "    if target_transform:\n",
        "        target_transform = normalize_text\n",
        "    else:\n",
        "        target_transform = None\n",
        "\n",
        "  # load data\n",
        "    global test_dataset\n",
        "    train_dataset, test_dataset = preProcess_datasets(data_dir)\n",
        "\n",
        "    full_training_data = RefCOCOgDataset(dataset=train_dataset, transform=transform_mat, target_transform=target_transform, data_dir=data_dir)\n",
        "\n",
        "\n",
        "\n",
        "    test_data = RefCOCOgDataset(dataset=test_dataset, transform=transform_mat, target_transform=target_transform, data_dir=data_dir)\n",
        "    num_samples = len(full_training_data)\n",
        "\n",
        "    training_samples = int(num_samples * 0.8)\n",
        "    validation_samples = num_samples - training_samples\n",
        "\n",
        "    training_data, validation_data = torch.utils.data.random_split(full_training_data, [training_samples, validation_samples])\n",
        "    print(len(training_data))\n",
        "\n",
        "    print(len(validation_data))\n",
        "    print(len(test_data))\n",
        "    # initialize dataloaders_collate\n",
        "    train_loader = torch.utils.data.DataLoader(training_data, batch_size, shuffle=True, num_workers=4, collate_fn=my_collate_fn)\n",
        "    val_loader = torch.utils.data.DataLoader(validation_data, test_batch_size, shuffle=False, num_workers=4, collate_fn=my_collate_fn)\n",
        "    test_loader = torch.utils.data.DataLoader(test_data, test_batch_size, shuffle=False, num_workers=4, collate_fn=my_collate_fn)\n",
        "\n",
        "    return train_loader, val_loader, test_loader\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V-TbnepNN8_u"
      },
      "outputs": [],
      "source": [
        "# tensorboard logging utilities\n",
        "def log_values(writer, step, loss, accuracy, prefix):\n",
        "  writer.add_scalar(f\"{prefix}/loss\", loss, step)\n",
        "  writer.add_scalar(f\"{prefix}/accuracy\", accuracy, step)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RmcsfGIf_1fd"
      },
      "outputs": [],
      "source": [
        "# main funcition\n",
        "def main(\n",
        "      root='/content/dataset/refcocog/',\n",
        "      data_dir='dataset/refcocog',\n",
        "      batch_size=128,\n",
        "      test_batch_size=128,\n",
        "      num_classes=4,\n",
        "      learning_rate=0.002,\n",
        "      weight_decay=1e-5,\n",
        "      eps=1e-8,\n",
        "      momentum=0.9,\n",
        "      epochs=10\n",
        "    ):\n",
        "    best_val_accuracy = 0.05  # Initialize to a low value\n",
        "    best_epoch = -1\n",
        "    writer = SummaryWriter(log_dir=\"runs/exp1\")\n",
        "    #global test_loader\n",
        "    # train clip on zero-shot learning and instantiates dataloaders\n",
        "    train_loader, val_loader, test_loader = get_data(data_dir=data_dir, batch_size=batch_size, test_batch_size=test_batch_size, transform=True, target_transform=True)\n",
        "\n",
        "    # instantiate the optimizer\n",
        "    optimizer = get_optimizer(modified_model, learning_rate, weight_decay, momentum, eps)\n",
        "\n",
        "    # define the cost function\n",
        "    cost_function = get_cost_function()\n",
        "\n",
        "    torch.cuda.empty_cache()\n",
        "    # computes evaluation results before training\n",
        "    print('Before training:')\n",
        "    train_loss, train_accuracy = test_step(modified_model, train_loader, cost_function)\n",
        "    torch.cuda.empty_cache()\n",
        "    val_loss, val_accuracy = test_step(modified_model, val_loader, cost_function)\n",
        "    torch.cuda.empty_cache()\n",
        "    test_loss, test_accuracy = test_step(modified_model, test_loader, cost_function)\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    # log to TensorBoard\n",
        "    log_values(writer, -1, train_loss, train_accuracy, \"train\")\n",
        "    log_values(writer, -1, val_loss, val_accuracy, \"validation\")\n",
        "    log_values(writer, -1, test_loss, test_accuracy, \"test\")\n",
        "\n",
        "    print('\\tTraining loss {:.5f}, Training accuracy {:.2f}'.format(train_loss, train_accuracy))\n",
        "    print('\\tValidation loss {:.5f}, Validation accuracy {:.2f}'.format(val_loss, val_accuracy))\n",
        "    print('\\tTest loss {:.5f}, Test accuracy {:.2f}'.format(test_loss, test_accuracy))\n",
        "    print('-----------------------------------------------------')\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    # for each epoch, train the network and then compute evaluation results\n",
        "    for e in range(epochs):\n",
        "        train_loss, train_accuracy = training_step(modified_model, train_loader, optimizer, cost_function)\n",
        "        torch.cuda.empty_cache()\n",
        "        val_loss, val_accuracy = test_step(modified_model, val_loader, cost_function)\n",
        "        torch.cuda.empty_cache()\n",
        "        # logs to TensorBoard\n",
        "        log_values(writer, e, val_loss, val_accuracy, \"Validation\")\n",
        "\n",
        "        print('Epoch: {:d}'.format(e+1))\n",
        "        print('\\tTraining loss {:.5f}, Training accuracy {:.2f}'.format(train_loss, train_accuracy))\n",
        "        print('\\tValidation loss {:.5f}, Validation accuracy {:.2f}'.format(val_loss, val_accuracy))\n",
        "        print('-----------------------------------------------------')\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "        # Check if current model has better validation accuracy\n",
        "        if val_accuracy > best_val_accuracy:\n",
        "            best_val_accuracy = val_accuracy\n",
        "            best_epoch = e\n",
        "            #save the model\n",
        "            torch.save(modified_model.state_dict(), 'best_model.pth')\n",
        "\n",
        "    # compute final evaluation results\n",
        "    print('After training:')\n",
        "\n",
        "    #load saved model\n",
        "    modified_model.load_state_dict(torch.load('best_model.pth'))\n",
        "\n",
        "    train_loss, train_accuracy = test_step(modified_model, train_loader, cost_function)\n",
        "    torch.cuda.empty_cache()\n",
        "    val_loss, val_accuracy = test_step(modified_model, val_loader, cost_function)\n",
        "    torch.cuda.empty_cache()\n",
        "    test_loss, test_accuracy = test_step(modified_model, test_loader, cost_function)\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    # log to TensorBoard\n",
        "    log_values(writer, epochs, train_loss, train_accuracy, \"train\")\n",
        "    log_values(writer, epochs, val_loss, val_accuracy, \"validation\")\n",
        "    log_values(writer, epochs, test_loss, test_accuracy, \"test\")\n",
        "\n",
        "    print('\\tTraining loss {:.5f}, Training accuracy {:.2f}'.format(train_loss, train_accuracy))\n",
        "    print('\\tValidation loss {:.5f}, Validation accuracy {:.2f}'.format(val_loss, val_accuracy))\n",
        "    print('\\tTest loss {:.5f}, Test accuracy {:.2f}'.format(test_loss, test_accuracy))\n",
        "    print('-----------------------------------------------------')\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "\n",
        "    # closes the logger\n",
        "    writer.close()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CdKpBW-vPQfr",
        "outputId": "397f9336-37d0-4dcd-ee55-5a954aca24c7",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4000\n",
            "1000\n",
            "5000\n",
            "Before training:\n",
            "\tTraining loss 0.86992, Training accuracy 0.04\n",
            "\tValidation loss 0.88128, Validation accuracy 0.04\n",
            "\tTest loss 0.87417, Test accuracy 0.04\n",
            "-----------------------------------------------------\n",
            "Epoch: 1\n",
            "\tTraining loss 0.44614, Training accuracy 0.05\n",
            "\tValidation loss 0.88128, Validation accuracy 0.04\n",
            "-----------------------------------------------------\n",
            "After training:\n",
            "\tTraining loss 0.86844, Training accuracy 0.04\n",
            "\tValidation loss 0.88128, Validation accuracy 0.04\n",
            "\tTest loss 0.87417, Test accuracy 0.04\n",
            "-----------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CTW32KCDQvVJ"
      },
      "outputs": [],
      "source": [
        "def draw_bbx(bb, color='r'):\n",
        "    bb = np.array(bb, dtype=np.float32)\n",
        "    print(bb)\n",
        "    return patches.Rectangle((bb[0], bb[1]), bb[2] - bb[0], bb[3] - bb[1],\n",
        "                         linewidth=2, edgecolor=color, facecolor='none')\n",
        "\n",
        "\n",
        "def inference(model, query, image, gd_bbox):\n",
        "\n",
        "    model.load_state_dict(torch.load('best_model.pth'))\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        fname = os.path.join(self.data_dir+'/images/', image)\n",
        "        image = Image.open(fname).convert('RGB')\n",
        "        img = transform_mat(image).float()\n",
        "        cap = clip.tokenize([normalize_text(sent) for sent in query]).float()\n",
        "\n",
        "        bbox = model(img.unsqueeze(0).to(device), cap.unsqueeze(0).to(device))\n",
        "        print('inference')\n",
        "        print(bbox)\n",
        "        print(bbox.shape)\n",
        "        out_bb = bbox.cpu().numpy()\n",
        "        #caption = tokenizer.decode(query[0], skip_special_tokens=False)\n",
        "\n",
        "        # Iterate over each bounding box\n",
        "        for i in range(out_bb.shape[0]):\n",
        "            current_bbox = out_bb[i]\n",
        "            # Display the image with bounding box\n",
        "            plt.figure()\n",
        "            img_to_show = cv2.cvtColor(image.numpy(), cv2.COLOR_BGR2RGB)\n",
        "            plt.imshow(img_to_show)  # Assuming image is a PyTorch tensor\n",
        "            plt.gca().add_patch(draw_bbx(current_bbox, 'r'))\n",
        "            plt.axis('off')\n",
        "            plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JzgeqEvWmnfv"
      },
      "outputs": [],
      "source": [
        "x = test_dataset[4]\n",
        "print(x['captions'])\n",
        "\n",
        "inference(modified_model,\n",
        "             query=x['captions'],\n",
        "             image=x['image'],\n",
        "              gd_bbox=x['bbox'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IZChWYiFGEWm"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
