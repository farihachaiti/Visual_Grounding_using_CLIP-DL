{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0SCUZq8R27Nv",
    "outputId": "96b738a7-e8e1-4b5f-b630-bd7f42900862"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘dataset’: File exists\n",
      "Requirement already satisfied: gdown in /home/disi/.local/lib/python3.8/site-packages (4.7.1)\n",
      "Requirement already satisfied: six in /usr/lib/python3/dist-packages (from gdown) (1.14.0)\n",
      "Requirement already satisfied: beautifulsoup4 in /home/disi/.local/lib/python3.8/site-packages (from gdown) (4.12.2)\n",
      "Requirement already satisfied: filelock in /home/disi/.local/lib/python3.8/site-packages (from gdown) (3.12.0)\n",
      "Requirement already satisfied: requests[socks] in /home/disi/.local/lib/python3.8/site-packages (from gdown) (2.31.0)\n",
      "Requirement already satisfied: tqdm in /home/disi/.local/lib/python3.8/site-packages (from gdown) (4.65.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in /home/disi/.local/lib/python3.8/site-packages (from beautifulsoup4->gdown) (2.5)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests[socks]->gdown) (2.8)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3/dist-packages (from requests[socks]->gdown) (1.25.8)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/disi/.local/lib/python3.8/site-packages (from requests[socks]->gdown) (3.3.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests[socks]->gdown) (2019.11.28)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6; extra == \"socks\" in /home/disi/.local/lib/python3.8/site-packages (from requests[socks]->gdown) (1.7.1)\n",
      "Downloading...\n",
      "From (uriginal): https://drive.google.com/uc?id=1xijq32XfEm6FPhUb7RsZYWHc2UuwVkiq\n",
      "From (redirected): https://drive.google.com/uc?id=1xijq32XfEm6FPhUb7RsZYWHc2UuwVkiq&confirm=t&uuid=7ce04594-263c-4aa6-90de-6d4fef7f2f35\n",
      "To: /home/disi/.jupyter/refcocog.tar.gz\n",
      "100%|███████████████████████████████████████| 13.5G/13.5G [01:25<00:00, 158MB/s]\n"
     ]
    }
   ],
   "source": [
    "#from google.colab import drive\n",
    "\n",
    "#drive.mount('/content/drive')\n",
    "!mkdir dataset\n",
    "!pip install gdown\n",
    "!gdown 1xijq32XfEm6FPhUb7RsZYWHc2UuwVkiq\n",
    "!mv refcocog.tar.gz ./dataset/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "8Bum1NKU2v5a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "refcocog  refcocog.tar.gz\r\n"
     ]
    }
   ],
   "source": [
    "!tar -xf dataset/refcocog.tar.gz -C dataset\n",
    "!ls dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LZum46_RLQ44",
    "outputId": "724658ba-a4bd-4633-f894-21cbd8f93124"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/openai/CLIP.git\n",
      "  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-jnk0akfq\n",
      "  Running command git clone -q https://github.com/openai/CLIP.git /tmp/pip-req-build-jnk0akfq\n",
      "Requirement already satisfied, skipping upgrade: ftfy in /home/disi/.local/lib/python3.8/site-packages (from clip==1.0) (6.1.3)\n",
      "Requirement already satisfied, skipping upgrade: regex in /home/disi/.local/lib/python3.8/site-packages (from clip==1.0) (2023.6.3)\n",
      "Requirement already satisfied, skipping upgrade: torch in /home/disi/.local/lib/python3.8/site-packages (from clip==1.0) (2.1.0.dev20230615+cu121)\n",
      "Requirement already satisfied, skipping upgrade: torchvision in /home/disi/.local/lib/python3.8/site-packages (from clip==1.0) (0.16.0.dev20230615+cu121)\n",
      "Requirement already satisfied, skipping upgrade: tqdm in /home/disi/.local/lib/python3.8/site-packages (from clip==1.0) (4.65.0)\n",
      "Requirement already satisfied, skipping upgrade: wcwidth<0.3.0,>=0.2.12 in /home/disi/.local/lib/python3.8/site-packages (from ftfy->clip==1.0) (0.2.13)\n",
      "Requirement already satisfied, skipping upgrade: filelock in /home/disi/.local/lib/python3.8/site-packages (from torch->clip==1.0) (3.12.0)\n",
      "Requirement already satisfied, skipping upgrade: fsspec in /home/disi/.local/lib/python3.8/site-packages (from torch->clip==1.0) (2023.4.0)\n",
      "Requirement already satisfied, skipping upgrade: networkx in /home/disi/.local/lib/python3.8/site-packages (from torch->clip==1.0) (3.1)\n",
      "Requirement already satisfied, skipping upgrade: typing-extensions in /home/disi/.local/lib/python3.8/site-packages (from torch->clip==1.0) (4.9.0)\n",
      "Requirement already satisfied, skipping upgrade: jinja2 in /home/disi/.local/lib/python3.8/site-packages (from torch->clip==1.0) (3.1.2)\n",
      "Requirement already satisfied, skipping upgrade: pytorch-triton==2.1.0+440fd1bf20 in /home/disi/.local/lib/python3.8/site-packages (from torch->clip==1.0) (2.1.0+440fd1bf20)\n",
      "Requirement already satisfied, skipping upgrade: sympy in /home/disi/.local/lib/python3.8/site-packages (from torch->clip==1.0) (1.12)\n",
      "Requirement already satisfied, skipping upgrade: pillow!=8.3.*,>=5.3.0 in /home/disi/.local/lib/python3.8/site-packages (from torchvision->clip==1.0) (9.5.0)\n",
      "Requirement already satisfied, skipping upgrade: numpy in /home/disi/.local/lib/python3.8/site-packages (from torchvision->clip==1.0) (1.24.4)\n",
      "Requirement already satisfied, skipping upgrade: requests in /home/disi/.local/lib/python3.8/site-packages (from torchvision->clip==1.0) (2.31.0)\n",
      "Requirement already satisfied, skipping upgrade: MarkupSafe>=2.0 in /home/disi/.local/lib/python3.8/site-packages (from jinja2->torch->clip==1.0) (2.1.3)\n",
      "Requirement already satisfied, skipping upgrade: mpmath>=0.19 in /home/disi/.local/lib/python3.8/site-packages (from sympy->torch->clip==1.0) (1.3.0)\n",
      "Requirement already satisfied, skipping upgrade: charset-normalizer<4,>=2 in /home/disi/.local/lib/python3.8/site-packages (from requests->torchvision->clip==1.0) (3.3.2)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->torchvision->clip==1.0) (2019.11.28)\n",
      "Requirement already satisfied, skipping upgrade: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->torchvision->clip==1.0) (2.8)\n",
      "Requirement already satisfied, skipping upgrade: urllib3<3,>=1.21.1 in /usr/lib/python3/dist-packages (from requests->torchvision->clip==1.0) (1.25.8)\n",
      "Building wheels for collected packages: clip\n",
      "  Building wheel for clip (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369522 sha256=0d20b1fc8f0d87537ecf6388ebfec1990b42113f19295c7d9bb1d125ed12c81a\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-__d3bwmo/wheels/ab/4f/3a/5e51521b55997aa6f0690e095c08824219753128ce8d9969a3\n",
      "Successfully built clip\n",
      "Installing collected packages: clip\n",
      "  Attempting uninstall: clip\n",
      "    Found existing installation: clip 1.0\n",
      "    Uninstalling clip-1.0:\n",
      "      Successfully uninstalled clip-1.0\n",
      "Successfully installed clip-1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade git+https://github.com/openai/CLIP.git\n",
    "\n",
    "from clip import clip\n",
    "\n",
    "model, preprocess = clip.load(\"RN50\")\n",
    "\n",
    "model = model.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:<1024>\"\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"]=\"1\"\n",
    "os.environ[\"TORCH_USE_CUDA_DSA\"]=\"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LdnLcbFyfkvW",
    "outputId": "587ab6d0-2660-4d0d-a487-9f958c37d1b1"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "from torchvision import ops\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from posixpath import split\n",
    "import json\n",
    "import tarfile\n",
    "import io\n",
    "import pickle\n",
    "from PIL import Image\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# import some common libraries\n",
    "import cv2\n",
    "#from google.colab.patches import cv2_imshow\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "data = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "PxhqoAtfkLaI"
   },
   "outputs": [],
   "source": [
    "def preProcess_datasets(data_dir):\n",
    "  train_dataset = {}\n",
    "  test_dataset = {}\n",
    "  f = open(f'{data_dir}/annotations/refs(umd).p', 'rb')\n",
    "  data['refs'] = pickle.load(f)\n",
    "  instances_file = os.path.join(f'{data_dir}/annotations/instances.json')\n",
    "  instances = json.load(open(instances_file, 'r'))\n",
    "  data['images'] = instances['images']\n",
    "  data['annotations'] = instances['annotations']\n",
    "\n",
    "  c=0\n",
    "  d=0\n",
    "  for key,val in enumerate(data['refs']):\n",
    "    for v in data['images']:\n",
    "      if val['image_id'] == v['id']:\n",
    "        fname = os.path.join(data_dir+'/images/', v['file_name'])\n",
    "        if os.path.exists(fname):\n",
    "          if val['split'] == 'train' or val['split'] == 'val':\n",
    "            train_dataset[c] = {}\n",
    "            train_dataset[c]['image'] = v['file_name']\n",
    "            train_dataset[c]['captions'] = [t['raw'] for t in val['sentences']]\n",
    "            train_dataset[c]['id'] = val['image_id']\n",
    "            break\n",
    "          else:\n",
    "            test_dataset[d] = {}\n",
    "            test_dataset[d]['image'] = v['file_name']\n",
    "            test_dataset[d]['captions'] = [t['raw'] for t in val['sentences']]\n",
    "            test_dataset[d]['id'] = val['image_id']\n",
    "            break\n",
    "    for ann in data['annotations']:\n",
    "      if val['ann_id'] == ann['id']:\n",
    "        if val['split'] == 'train' or val['split'] == 'val':\n",
    "          train_dataset[c]['bbox'] = ann['bbox']\n",
    "          c+=1\n",
    "          break\n",
    "        else:\n",
    "          test_dataset[d]['bbox'] = ann['bbox']\n",
    "          d+=1\n",
    "          break\n",
    "    if c>=5000:\n",
    "      break\n",
    "\n",
    "  return train_dataset, test_dataset\n",
    "\n",
    "\n",
    "def resize_bounding_box(bbox, original_size, new_size):\n",
    "  x_min, y_min, x_max, y_max = bbox\n",
    "  orig_width, orig_height = original_size\n",
    "  new_width, new_height = new_size\n",
    "\n",
    "  # Calculate the scaling factors\n",
    "  width_ratio = new_width / orig_width\n",
    "  height_ratio = new_height / orig_height\n",
    "\n",
    "  # Resize the bounding box coordinates\n",
    "  new_x_min = int(x_min * width_ratio)\n",
    "  new_y_min = int(y_min * height_ratio)\n",
    "  new_x_max = int(x_max * width_ratio)\n",
    "  new_y_max = int(y_max * height_ratio)\n",
    "\n",
    "  return new_x_min, new_y_min, new_x_max, new_y_max\n",
    "\n",
    "def xywh_to_xyxy(boxes):\n",
    "  xmin = boxes[0]\n",
    "  ymin = boxes[1]\n",
    "  w = boxes[2]\n",
    "  h = boxes[3]\n",
    "  xmax = xmin + w\n",
    "  ymax = ymin + h\n",
    "  return [xmin, ymin, xmax, ymax]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "2b4mMLT8vbdE"
   },
   "outputs": [],
   "source": [
    "class RefCOCOgDataset(Dataset):\n",
    "    def __init__(self, dataset, transform, target_transform, data_dir='dataset/refcocog'):\n",
    "        super(RefCOCOgDataset, self).__init__()\n",
    "        self.data_dir = data_dir\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.dataset = dataset\n",
    "\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data_item = {}\n",
    "        fname = os.path.join(self.data_dir+'/images/', self.dataset[idx]['image'])\n",
    "        image = Image.open(fname).convert('RGB')\n",
    "        shape_0 = list(image.size)\n",
    "        if self.transform:\n",
    "            data_item['image'] = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            data_item['captions'] = clip.tokenize([sent for desc in self.dataset[idx]['captions'] for sent in desc])\n",
    "            bb = xywh_to_xyxy(self.dataset[idx]['bbox'])\n",
    "            bbox = resize_bounding_box(bb, shape_0, [224,224])\n",
    "            data_item['bbox'] = torch.tensor(bbox)\n",
    "        return data_item\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "nB6aV1uyGAxd"
   },
   "outputs": [],
   "source": [
    "def get_cost_function():\n",
    "  cost_function = torch.nn.SmoothL1Loss()\n",
    "  return cost_function\n",
    "\n",
    "def get_iou_torch(ground_truth, pred):\n",
    "  # Coordinates of the area of intersection.\n",
    "\n",
    "  ix1 = torch.max(ground_truth[0][0], pred[0][0])\n",
    "  iy1 = torch.max(ground_truth[0][1], pred[0][1])\n",
    "  ix2 = torch.min(ground_truth[0][2], pred[0][2])\n",
    "  iy2 = torch.min(ground_truth[0][3], pred[0][3])\n",
    "\n",
    "  # Intersection height and width.\n",
    "  i_height = torch.max(iy2 - iy1 + 1, torch.tensor(0.))\n",
    "  i_width = torch.max(ix2 - ix1 + 1, torch.tensor(0.))\n",
    "\n",
    "  area_of_intersection = i_height * i_width\n",
    "\n",
    "  # Ground Truth dimensions.\n",
    "  gt_height = ground_truth[0][3] - ground_truth[0][1] + 1\n",
    "  gt_width = ground_truth[0][2] - ground_truth[0][0] + 1\n",
    "\n",
    "  # Prediction dimensions.\n",
    "  pd_height = pred[0][3] - pred[0][1] + 1\n",
    "  pd_width = pred[0][2] - pred[0][0] + 1\n",
    "\n",
    "  area_of_union = gt_height * gt_width + pd_height * pd_width - area_of_intersection\n",
    "\n",
    "  iou = (area_of_intersection + 1e-5) / (area_of_union + 1e-5) # Adding a small epsilon to avoid division by zero\n",
    "  #average_iou = torch.mean(iou)\n",
    "\n",
    "  return iou.detach().cpu().numpy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "XFYNX6i_GA-O"
   },
   "outputs": [],
   "source": [
    "def get_optimizer(model, lr, wd, momentum):\n",
    "  optimizer = torch.optim.SGD([\n",
    "      {'params': model.parameters(), 'lr': lr}\n",
    "  ], lr=lr, weight_decay=wd, momentum=momentum)\n",
    "\n",
    "  #optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "  return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "TIeelz-AF32o"
   },
   "outputs": [],
   "source": [
    "def training_step(net, data_loader, optimizer, cost_function, device='cuda:0'):\n",
    "    samples = 0.0\n",
    "    cumulative_loss = 0.0\n",
    "    cumulative_accuracy = 0.0\n",
    "    batch_ious = []\n",
    "    # set the network to training mode\n",
    "    net = net.float()\n",
    "    net.train()\n",
    "    accumulation_steps = 2\n",
    "    clip_value = 1.0\n",
    "  # iterate over the training set\n",
    "    for batch_idx, batch in enumerate(data_loader):\n",
    "\n",
    "      #load data into GPU\n",
    "      batch['image'] = batch['image'].to(device)\n",
    "      batch['captions'] = batch['captions'].to(device)\n",
    "      target_bbox = batch['bbox'].to(device)\n",
    "      # forward pass\n",
    "      out_bbox = net(batch['image'], batch['captions'])\n",
    "      out_bbox = out_bbox.to(device)\n",
    "      # loss computation\n",
    "      loss = cost_function(out_bbox, target_bbox)\n",
    "      # fetch prediction and loss value\n",
    "      # backward pass\n",
    "      loss.requires_grad = True\n",
    "      loss.backward()\n",
    "        \n",
    "      if (batch_idx + 1) % accumulation_steps == 0:\n",
    "          # parameters update\n",
    "          optimizer.step()\n",
    "          # gradients reset\n",
    "          optimizer.zero_grad()\n",
    "          torch.nn.utils.clip_grad_norm_(net.parameters(), clip_value)\n",
    "      # fetch prediction and loss value\n",
    "      samples += batch['bbox'].shape[0]\n",
    "        \n",
    "      cumulative_loss += loss.item()\n",
    "\n",
    "      iou_accuracy = torchvision.ops.box_iou(target_bbox, out_bbox).detach().cpu().numpy().sum().item()\n",
    "\n",
    "      batch_ious.append(iou_accuracy)\n",
    "    if batch_idx % accumulation_steps != 0:\n",
    "        # parameters update\n",
    "        optimizer.step()\n",
    "        # gradients reset\n",
    "        optimizer.zero_grad()\n",
    "        torch.nn.utils.clip_grad_norm_(net.parameters(), clip_value)\n",
    "      # Calculate the average IoU across all batches\n",
    "    if batch_ious:\n",
    "        # Calculate the average IoU across all batches\n",
    "        cumulative_accuracy = np.mean(batch_ious)\n",
    "    else:\n",
    "        cumulative_accuracy = 0.0 # Set to zero if there are no elements\n",
    "\n",
    "\n",
    "    return cumulative_loss/samples, cumulative_accuracy\n",
    "\n",
    "def test_step(net, data_loader, cost_function, device='cuda:0'):\n",
    "    samples = 0.0\n",
    "    cumulative_loss = 0.0\n",
    "    cumulative_accuracy = 0.0\n",
    "    batch_ious = []\n",
    "\n",
    "    net = net.float()\n",
    "    # set the network to evaluation mode\n",
    "    net.eval()\n",
    "\n",
    "    # disable gradient computation (we are only testing, we do not want our model to be modified in this step!)\n",
    "    with torch.no_grad():\n",
    "        # iterate over the test set\n",
    "\n",
    "      for batch_idx, batch in enumerate(data_loader):\n",
    "\n",
    "        #load data into GPU\n",
    "        batch['image'] = batch['image'].to(device)\n",
    "        batch['captions'] = batch['captions'].to(device)\n",
    "        target_bbox = batch['bbox'].to(device)\n",
    "\n",
    "        # forward pass\n",
    "        out_bbox = net(batch['image'], batch['captions'])\n",
    "        out_bbox = out_bbox.to(device)\n",
    "        \n",
    "        # loss computation on GPU\n",
    "        loss = cost_function(out_bbox, target_bbox)\n",
    "\n",
    "        # accumulate the loss on the CPU\n",
    "        cumulative_loss += loss.item()\n",
    "\n",
    "        samples += batch['bbox'].shape[0]\n",
    "\n",
    "        iou_accuracy = torchvision.ops.box_iou(target_bbox, out_bbox).detach().cpu().numpy().sum().item()\n",
    "        batch_ious.append(iou_accuracy)\n",
    "\n",
    "        # Calculate the average IoU across all batches\n",
    "        if batch_ious:\n",
    "            cumulative_accuracy = np.mean(batch_ious)\n",
    "        else:\n",
    "            cumulative_accuracy = 0.0  # Set to zero if there are no elements\n",
    "\n",
    "    # Move the final cumulative loss back to the GPU\n",
    "    #cumulative_loss = torch.tensor(cumulative_loss).to(device)\n",
    "\n",
    "    return cumulative_loss / samples, cumulative_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "t2f4f07KFLoF"
   },
   "outputs": [],
   "source": [
    "def _make_grid(input):\n",
    "    # Get the width and height of the output feature map\n",
    "    _, width, height = input.size()\n",
    "    # Determine the size of each grid cell\n",
    "    grid_size = height // 16\n",
    "    grid_cells = []\n",
    "    grid_coordinates = []\n",
    "    for i in range(16):\n",
    "        for j in range(16):\n",
    "            # Calculate the coordinates for the current grid cell\n",
    "            x1 = i * grid_size\n",
    "            y1 = j * grid_size\n",
    "            x2 = x1 + grid_size\n",
    "            y2 = y1 + grid_size\n",
    "            # Extract the region corresponding to the grid cell\n",
    "            grid_cell = input[:, x1:x2, y1:y2]\n",
    "            grid_coordinates.append(torch.tensor([float(x1),float(y1),float(x2),float(y2)]))\n",
    "            grid_cells.append(grid_cell)\n",
    "    grid_cells = torch.stack(grid_cells)\n",
    "    grid_coordinates = torch.stack(grid_coordinates)\n",
    "    return grid_cells, grid_coordinates\n",
    "\n",
    "\n",
    "class Grid(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Grid, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        res = []\n",
    "        for img in x:\n",
    "            res.append(_make_grid(img))\n",
    "        return res\n",
    "\n",
    "\n",
    "class CustomCLIP(torch.nn.Module):\n",
    "  def __init__(self, model, num_classes: int = 4, bias=False):\n",
    "    super().__init__()\n",
    "    self.model = model\n",
    "    self.num_classes = num_classes\n",
    "    self.encoder = self.model.visual.float()\n",
    "    self.conv1 = self.encoder.conv1\n",
    "    self.bn1 = self.encoder.bn1\n",
    "    self.relu1 = self.encoder.relu1\n",
    "    self.avgpool = self.encoder.avgpool\n",
    "    self.fc = torch.nn.Linear(512, self.num_classes, bias=bias)\n",
    "    self.relu = torch.nn.ReLU()\n",
    "\n",
    "\n",
    "    self.text_encoder = self.encode_text\n",
    "\n",
    "    self.grid = Grid()\n",
    "\n",
    "    # add a bottleneck\n",
    "    self.image_encoder = torch.nn.Sequential(\n",
    "      self.conv1,\n",
    "      self.bn1,\n",
    "      self.relu1,\n",
    "      self.avgpool,\n",
    "      torch.nn.Flatten(),\n",
    "      torch.nn.Linear(288, self.num_classes, bias=bias),\n",
    "    )\n",
    "\n",
    "\n",
    "  def encode_text(self, text):\n",
    "      x = self.model.token_embedding(text)\n",
    "      x = self.fc(x)\n",
    "      x = x[:, :, :, :4]\n",
    "      return x\n",
    "\n",
    "  def forward(self, img, cap):\n",
    "    grids = self.grid(img)\n",
    "    aspect_ratio = 224 / 224\n",
    "    target_width = 224 // 16\n",
    "    target_height = int(target_width / aspect_ratio)\n",
    "    similar = []\n",
    "    for idx, (gd, _) in enumerate(grids):\n",
    "      x_ = []\n",
    "      for g in gd:\n",
    "          x = F.interpolate(g.unsqueeze(0), size=(target_width, target_height), mode='bilinear', align_corners=False)\n",
    "          x = self.image_encoder(x)\n",
    "          x /= x.norm(dim=-1, keepdim=True)\n",
    "          x_.append(x)\n",
    "      #with torch.no_grad():\n",
    "      y = self.text_encoder(cap[idx].unsqueeze(0))\n",
    "      y /= y.norm(dim=-1, keepdim=True)\n",
    "\n",
    "\n",
    "      im_ = torch.cat(x_)\n",
    "\n",
    "      target_len = y.size(2) - im_.size(0)\n",
    "      im_ = F.pad(im_, (0,0,0,target_len), value=0)\n",
    "\n",
    "\n",
    "      similarity_scores = torch.nn.functional.cosine_similarity(im_, y, dim=-1)\n",
    "      max_values, max_indices = torch.max(similarity_scores, dim=1)\n",
    "      boxes = grids[idx][1]\n",
    "      selected_box = boxes[max_indices.to(boxes.device)]\n",
    "\n",
    "      similar.append(selected_box)\n",
    "\n",
    "    out_bbox = torch.cat(similar)\n",
    "    out_bbox = out_bbox[:, 0, :]\n",
    "\n",
    "    return out_bbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "MGHhKD_Tgpjh"
   },
   "outputs": [],
   "source": [
    "def pad_sequence(batch):\n",
    "  return torch.nn.utils.rnn.pad_sequence(batch, batch_first=True, padding_value=0)\n",
    "def my_collate_fn(batch):\n",
    "\n",
    "  return {\n",
    "      'image': pad_sequence([x['image'] for x in batch]),\n",
    "      'captions': pad_sequence([x['captions'] for x in batch]),\n",
    "      'bbox': pad_sequence([x['bbox'] for x in batch])\n",
    "      }\n",
    "\n",
    "\n",
    "def get_data(data_dir, batch_size, test_batch_size, transform=True, target_transform=False):\n",
    "\n",
    "\n",
    "    if transform:\n",
    "        # convert the PIL images to Tensors\n",
    "        transform = T.Compose([\n",
    "            T.Resize((224, 224)),\n",
    "            T.RandomCrop(size=224),\n",
    "            T.RandomHorizontalFlip(),\n",
    "            T.RandomRotation(degrees=30),\n",
    "            T.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),\n",
    "            T.ToTensor(),\n",
    "            T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ])\n",
    "    else:\n",
    "          # prepare data transformations and then combine them sequentially\n",
    "        transform = None\n",
    "    if target_transform:\n",
    "        target_transform = T.Compose([\n",
    "                                 lambda y: torch.zeros(10, dtype=torch.float).scatter_(0, torch.tensor(y), value=1)])\n",
    "    else:\n",
    "        target_transform = None\n",
    "\n",
    "  # load data\n",
    "    train_dataset, test_dataset = preProcess_datasets(data_dir)\n",
    "\n",
    "    full_training_data = RefCOCOgDataset(dataset=train_dataset, transform=transform, target_transform=target_transform, data_dir=data_dir)\n",
    "\n",
    "\n",
    "    global test_data\n",
    "    test_data = RefCOCOgDataset(dataset=test_dataset, transform=transform, target_transform=target_transform, data_dir=data_dir)\n",
    "    num_samples = len(full_training_data)\n",
    "\n",
    "    training_samples = int(num_samples * 0.8 + 1)\n",
    "    validation_samples = num_samples - training_samples\n",
    "\n",
    "    training_data, validation_data = torch.utils.data.random_split(full_training_data, [training_samples, validation_samples])\n",
    "    print(len(training_data))\n",
    "\n",
    "    print(len(validation_data))\n",
    "    print(len(test_data))\n",
    "  # initialize dataloaders_collate\n",
    "    train_loader = torch.utils.data.DataLoader(training_data, batch_size, shuffle=True, num_workers=2, collate_fn=my_collate_fn)\n",
    "    val_loader = torch.utils.data.DataLoader(validation_data, test_batch_size, shuffle=False, num_workers=2, collate_fn=my_collate_fn)\n",
    "    test_loader = torch.utils.data.DataLoader(test_data, test_batch_size, shuffle=False, num_workers=2, collate_fn=my_collate_fn)\n",
    "\n",
    "    return train_loader, val_loader, test_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "V-TbnepNN8_u"
   },
   "outputs": [],
   "source": [
    "# tensorboard logging utilities\n",
    "def log_values(writer, step, loss, accuracy, prefix):\n",
    "  writer.add_scalar(f\"{prefix}/loss\", loss, step)\n",
    "  writer.add_scalar(f\"{prefix}/accuracy\", accuracy, step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "RmcsfGIf_1fd"
   },
   "outputs": [],
   "source": [
    "# main funcition\n",
    "def main(\n",
    "      root='/content/dataset/refcocog/',\n",
    "      data_dir='dataset/refcocog',\n",
    "      batch_size=64,\n",
    "      test_batch_size=224,\n",
    "      num_classes=4,\n",
    "      learning_rate=0.1,\n",
    "      weight_decay=1e-5,\n",
    "      momentum=0.9,\n",
    "      epochs=10\n",
    "      #epochs=3\n",
    "    ):\n",
    "  writer = SummaryWriter(log_dir=\"runs/exp1\")\n",
    "  #global test_loader\n",
    "  # train clip on zero-shot learning and instantiates dataloaders\n",
    "  train_loader, val_loader, test_loader = get_data(data_dir=data_dir, batch_size=batch_size, test_batch_size=test_batch_size, transform=True, target_transform=True)\n",
    "\n",
    "\n",
    "  # instantiate the network and move it to the chosen device (GPU)\n",
    "  \n",
    "  global modified_model\n",
    "  modified_model = CustomCLIP(model=model, num_classes=num_classes).to(device)\n",
    "\n",
    "  # instantiate the optimizer\n",
    "  optimizer = get_optimizer(modified_model, learning_rate, weight_decay, momentum)\n",
    "\n",
    "  # define the cost function\n",
    "  cost_function = get_cost_function()\n",
    "\n",
    "  torch.cuda.empty_cache()\n",
    "  # computes evaluation results before training\n",
    "  print('Before training:')\n",
    "  train_loss, train_accuracy = test_step(modified_model, train_loader, cost_function)\n",
    "  torch.cuda.empty_cache()\n",
    "  val_loss, val_accuracy = test_step(modified_model, val_loader, cost_function)\n",
    "  torch.cuda.empty_cache()\n",
    "  test_loss, test_accuracy = test_step(modified_model, test_loader, cost_function)\n",
    "  torch.cuda.empty_cache()\n",
    "  \n",
    "  # log to TensorBoard\n",
    "  log_values(writer, -1, train_loss, train_accuracy, \"train\")\n",
    "  log_values(writer, -1, val_loss, val_accuracy, \"validation\")\n",
    "  log_values(writer, -1, test_loss, test_accuracy, \"test\")\n",
    "\n",
    "  print('\\tTraining loss {:.5f}, Training accuracy {:.2f}'.format(train_loss, train_accuracy))\n",
    "  print('\\tValidation loss {:.5f}, Validation accuracy {:.2f}'.format(val_loss, val_accuracy))\n",
    "  print('\\tTest loss {:.5f}, Test accuracy {:.2f}'.format(test_loss, test_accuracy))\n",
    "  print('-----------------------------------------------------')\n",
    "  torch.cuda.empty_cache()\n",
    "  # for each epoch, train the network and then compute evaluation results\n",
    "  for e in range(epochs):\n",
    "\n",
    "    train_loss, train_accuracy = training_step(modified_model, train_loader, optimizer, cost_function)\n",
    "    torch.cuda.empty_cache()\n",
    "    val_loss, val_accuracy = test_step(modified_model, val_loader, cost_function)\n",
    "    torch.cuda.empty_cache()\n",
    "    # logs to TensorBoard\n",
    "    log_values(writer, e, val_loss, val_accuracy, \"Validation\")\n",
    "\n",
    "    print('Epoch: {:d}'.format(e+1))\n",
    "    print('\\tTraining loss {:.5f}, Training accuracy {:.2f}'.format(train_loss, train_accuracy))\n",
    "    print('\\tValidation loss {:.5f}, Validation accuracy {:.2f}'.format(val_loss, val_accuracy))\n",
    "    print('-----------------------------------------------------')\n",
    "    torch.cuda.empty_cache()\n",
    "    # compute final evaluation results\n",
    "    print('After training:')\n",
    "    train_loss, train_accuracy = test_step(modified_model, train_loader, cost_function)\n",
    "    torch.cuda.empty_cache()\n",
    "    val_loss, val_accuracy = test_step(modified_model, val_loader, cost_function)\n",
    "    torch.cuda.empty_cache()\n",
    "    test_loss, test_accuracy = test_step(modified_model, test_loader, cost_function)\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # log to TensorBoard\n",
    "    log_values(writer, epochs, train_loss, train_accuracy, \"train\")\n",
    "    log_values(writer, epochs, val_loss, val_accuracy, \"validation\")\n",
    "    log_values(writer, epochs, test_loss, test_accuracy, \"test\")\n",
    "\n",
    "    print('\\tTraining loss {:.5f}, Training accuracy {:.2f}'.format(train_loss, train_accuracy))\n",
    "    print('\\tValidation loss {:.5f}, Validation accuracy {:.2f}'.format(val_loss, val_accuracy))\n",
    "    print('\\tTest loss {:.5f}, Test accuracy {:.2f}'.format(test_loss, test_accuracy))\n",
    "    print('-----------------------------------------------------')\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "  # closes the logger\n",
    "  writer.close()\n",
    "  torch.save(modified_model.state_dict(), 'model.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CdKpBW-vPQfr",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4001\n",
      "999\n",
      "5023\n",
      "Before training:\n",
      "\tTraining loss 1.71613, Training accuracy 3.28\n",
      "\tValidation loss 0.76210, Validation accuracy 17.32\n",
      "\tTest loss 0.71806, Test accuracy 14.03\n",
      "-----------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CTW32KCDQvVJ"
   },
   "outputs": [],
   "source": [
    "def draw_corner_rect(bb, color='red'):\n",
    "    bb = np.array(bb, dtype=np.float32)\n",
    "    print(bb)\n",
    "    return plt.Rectangle((bb[1], bb[0]), bb[3]-bb[1], bb[2]-bb[0], color=color,\n",
    "                         fill=False, lw=3)\n",
    "\n",
    "def draw_bbx(bb, color):\n",
    "    plt.gca().add_patch(draw_corner_rect(bb, color))\n",
    "\n",
    "\n",
    "def inference(model, query, image, gd_bbox):\n",
    "\n",
    "    model.load_state_dict(torch.load('model.pth'))\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "      bbox = model(image.unsqueeze(0).to(device), query.unsqueeze(0).to(device))\n",
    "      print('inference')\n",
    "      print(bbox)\n",
    "      print(bbox.shape)\n",
    "\n",
    "      #caption = tokenizer.decode(query[0], skip_special_tokens=False)\n",
    "\n",
    "\n",
    "      image = image.permute(1,2,0)\n",
    "      img = cv2.cvtColor(image.numpy(), cv2.COLOR_BGR2RGB)\n",
    "\n",
    "      out_bb = bbox.detach().cpu().numpy()\n",
    "      out_bb = out_bb.astype(int)\n",
    "      print(out_bb)\n",
    "      print(out_bb.shape)\n",
    "      plt.imshow(img)\n",
    "\n",
    "      draw_bbx(out_bb[0],'red')\n",
    "\n",
    "      plt.axis('off')\n",
    "      plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JzgeqEvWmnfv"
   },
   "outputs": [],
   "source": [
    "x = test_data[4]\n",
    "\n",
    "\n",
    "inference(modified_model,\n",
    "             query=x['captions'],\n",
    "             image=x['image'],\n",
    "              gd_bbox=x['bbox'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
